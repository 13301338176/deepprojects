{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keras version: 2.1.5\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy as sp\n",
    "\n",
    "from scipy import sparse\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "import string\n",
    "import re\n",
    "import glob\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_extraction.text import CountVectorizer, FeatureHasher\n",
    "\n",
    "import keras\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.layers import Dense, Embedding, LSTM, Dropout\n",
    "from keras.models import Sequential\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print('Keras version: %s' % keras.__version__)\n",
    "\n",
    "PATH = \"data/aclImdb\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# or use nltk or spacy\n",
    "htmltag = re.compile(r'<.*?>')\n",
    "numbers = re.compile(r'[0-9]')\n",
    "quotes = re.compile(r'\\\"|`')\n",
    "punctuation = re.compile(r'([%s])'% string.punctuation)\n",
    "english_stopwords =set(stopwords.words('english'))\n",
    "stemmer = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# read files in the given tree, using subfolders as the target classes\n",
    "def read_files(folder, subfolders):\n",
    "    corpus, labels = [], []\n",
    "    for index, label in enumerate(subfolders):\n",
    "        path = '/'.join([folder, label, '*.txt'])\n",
    "        for filename in glob.glob(path):\n",
    "            corpus.append(open(filename, 'r').read())\n",
    "            labels.append(index)\n",
    "    return corpus, np.array(labels).astype(np.int)\n",
    "\n",
    "# pre-processor\n",
    "def preprocess(s):\n",
    "    # lowercase\n",
    "    s = s.lower()\n",
    "    # remove html tags\n",
    "    s = htmltag.sub(' ', s)\n",
    "    # remove numbers\n",
    "    s = numbers.sub(' ', s)\n",
    "    # remove quotes\n",
    "    s = quotes.sub(' ', s)\n",
    "    # replace puctuation\n",
    "    s = punctuation.sub(' ', s)\n",
    "    return s\n",
    "    \n",
    "# tokenization\n",
    "def tokenize(s):\n",
    "    # use a serious tokenizer\n",
    "    tokens = nltk.word_tokenize(s)\n",
    "    # remove stopwords\n",
    "    tokens = filter(lambda w: not w in english_stopwords, tokens)\n",
    "    # stem words\n",
    "    tokens = [stemmer.stem(token) for token in tokens]\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#coprus_train_pos = [open(filename, 'r').read() for filename in glob.glob(PATH + '/train/pos/*.txt')]\n",
    "#coprus_train_neg = [open(filename, 'r').read() for filename in glob.glob(PATH + '/train/neg/*.txt')]\n",
    "corpus_train, y_train = read_files(PATH + '/train', ['neg', 'pos'])\n",
    "corpus_test, y_test = read_files(PATH + '/test', ['neg', 'pos'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25000,\n",
       " 25000,\n",
       " 'Hi, Everyone, If you saw \"Singing in the Rain,\" you remember the scene of Gene Kelly dancing in the rain. You also remember the dance number of Donald O\\'Connor, \"Make \\'em Laugh.\" If you saw \"Royal Wedding,\" you will remember Fred Astaire dancing on the ceiling. If you saw \"Jailhouse Rock,\" you will even remember the title dance number choreographed by The King himself.<br /><br />That is what is missing here. There could have been some blockbuster dance numbers in this presentation. The closest was Chuck McGowan\\'s \"I Can Do That.\" the mere fact that you have some talented people on stage moving together does not make a great dance film. Richard Attenborough was to blame for this failure. He pointed the camera at the stage and thought that would be a good thing.<br /><br />Yelling at people auditioning for a part in a Broadway production is not entertainment. Michael Douglas would be just as badly cast if he were in a Western or a comedy. He is OK when he is in a Michael Douglas movie where we see him yelling at someone we would like to yell at. It does not work here.<br /><br />The cast was good except for Michael, of course. A good movie could have been made even using the songs that were in the stage production, but someone should have thought about how to film it.<br /><br />Next time they do one of these I hope they call me first.<br /><br />Tom Willett',\n",
       " 0,\n",
       " 'One of the very best Three Stooges shorts ever. A spooky house full of evil guys and \"The Goon\" challenge the Alert Detective Agency\\'s best men. Shemp is in top form in the famous in-the-dark scene. Emil Sitka provides excellent support in his Mr. Goodrich role, as the target of a murder plot. Before it\\'s over, Shemp\\'s \"trusty little shovel\" is employed to great effect. This 16 minute gem moves about as fast as any Stooge\\'s short and packs twice the wallop. Highly recommended.',\n",
       " 1)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(corpus_train), len(y_train), corpus_train[0], y_train[0], corpus_train[24999], y_train[24999]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25000,\n",
       " 25000,\n",
       " 'Yes, in this movie you are treated to multiple little snowmen on the attack in apparently a very warm climate so yes this movie is definitely not to be taken seriously. It is in fact a much worse movie than the original as at least with that one the whole production looked like it cost more than a couple of bucks and a video camera to make. It has its funny moments, but really playing off the cheapness of your movie and making that be your intended laughs is kind of weak film making if you ask me. You can not come up with a good story, your effects are going to really be bad, hey let us just make the movie look as bad as possible with horrible one liners and we have our movie. The first one at least had a somewhat credible story as the snowman in that one attacked during the winter and not what amounts to a resort. It also had better effects too, this one is just a step or two ahead of \"Hobgoblins\" as far as the monsters are concerned and you really want to be more than a step a two above a bunch of hand puppets. Still, it makes up for all of this with a super ending that depicts a great sea vessel being taken out by the mighty frost. Actually, I am just kidding, but really it was the funniest part of the movie.',\n",
       " 0)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(corpus_test), len(y_test), corpus_test[0], y_test[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer(preprocessor=preprocess, tokenizer=tokenize)\n",
    "term_doc_train = vectorizer.fit_transform(corpus_train)\n",
    "term_doc_test = vectorizer.transform(corpus_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'abilityof', u'abishai']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab = vectorizer.get_feature_names()\n",
    "vocab[100:102]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((2, 10), array([[-1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "        [ 0., -1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.]]))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_size = len(vocab)\n",
    "h = FeatureHasher(n_features=10, input_type='string')\n",
    "f = h.fit_transform(['q', 'w'])\n",
    "f.shape, f.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<1x50440 sparse matrix of type '<type 'numpy.int64'>'\n",
       "\twith 85 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "term_doc_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, ..., 0, 0, 0]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "term_doc_train[100].toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9133"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer.vocabulary_['cool']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Multinomial Naive Bayes\n",
    "alpha = 0.1 # smoothing parameter\n",
    "class MultinomialNaiveBayes():\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "        alpha: smoothing parameter\n",
    "    \"\"\"\n",
    "    def __init__(self, alpha=0.1):\n",
    "        self.b = 0\n",
    "        self.r = 0\n",
    "        self.alpha = alpha\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        # bias\n",
    "        N_pos = (y==1).shape[0]\n",
    "        N_neg = (y==0).shape[0]\n",
    "        self.b = np.log(N_pos / N_neg)\n",
    "        # count of occurences for every token in vocabulary as they appear in positive samples\n",
    "        p = alpha + X[y==1].sum(axis=0)\n",
    "        p_l1 = np.linalg.norm(p, ord=1) # L1 norm\n",
    "        # count of occurences for every token in vocabulary as they appear in negative samples\n",
    "        q = alpha + X[y==0].sum(axis=0)\n",
    "        q_l1 = np.linalg.norm(q, ord=1) # L1 norm\n",
    "        # log count ratio\n",
    "        self.r = np.log((p/p_l1) / (q/q_l1))\n",
    "        #self.r = sp.sparse.csr_matrix(self.r.T)\n",
    "        return self.r, self.b\n",
    "\n",
    "    def predict(self, X):\n",
    "        y_pred = np.sign(sp.sparse.csr_matrix.dot(X, self.r.T) + self.b)\n",
    "        y_pred[y_pred==-1] = 0\n",
    "        return y_pred\n",
    "    \n",
    "    def score(self, X, y):\n",
    "        y_predict = self.predict(X)\n",
    "        y_reshaped = np.reshape(y, y_predict.shape)\n",
    "        return (y_reshaped == y_predict).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.0, (1, 50440), (25000, 50440))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = MultinomialNaiveBayes()\n",
    "r, b = model.fit(term_doc_train, y_train)\n",
    "b, r.shape, term_doc_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((25000, 50440),\n",
       " (1, 50440),\n",
       " <1x50440 sparse matrix of type '<type 'numpy.int64'>'\n",
       " \twith 85 stored elements in Compressed Sparse Row format>,\n",
       " matrix([[ 2.53558105, -2.26020949, -0.69100689, ...,  2.53558105,\n",
       "           0.13768578, -4.1249941 ]]))"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "term_doc_train.shape, r.shape, term_doc_train[0], r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.78408"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# accuracy on training set\n",
    "y_pred = model.predict(term_doc_train)\n",
    "#y_train = np.reshape(y_train, (25000, 1))\n",
    "(np.reshape(y_train, (25000, 1)) == y_pred).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.68644000000000005"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# accuracy on validation set\n",
    "y_pred2 = model.predict(term_doc_test)\n",
    "#y_test = np.reshape(y_test, (25000, 1))\n",
    "(np.reshape(y_test, (25000, 1)) == y_pred2).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((25000, 50440), (25000, 50440))"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# now let's binary term document\n",
    "term_doc_train = term_doc_train.sign() # turn everything into 1 or 0\n",
    "term_doc_test = term_doc_test.sign() # turn everything into 1 or 0\n",
    "term_doc_train.shape, term_doc_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.76848000000000005, 0.67620000000000002)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = MultinomialNaiveBayes()\n",
    "model.fit(term_doc_train, y_train)\n",
    "accuracy_train = model.score(term_doc_train, y_train)\n",
    "accuracy_test = model.score(term_doc_test, y_test)\n",
    "accuracy_train, accuracy_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((25000, 50440), (25000,), (1, 50440), (1, 50440))"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "term_doc_train.shape, y_train.shape, term_doc_train[y_train==0].sum(axis=0).shape, term_doc_train[y_train==1].sum(axis=0).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((25000,), (25000,), (25000, 1))"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(y_train==0).shape, (y_train==1).shape, y_pred.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.99187999999999998, 0.85855999999999999)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# now with plain logistic regression\n",
    "model = LogisticRegression()\n",
    "model.fit(term_doc_train, y_train)\n",
    "# accuracy on training\n",
    "y_pred = model.predict(term_doc_train)\n",
    "accuracy_train = (y_train == y_pred).mean()\n",
    "# accuracy on validation\n",
    "y_pred = model.predict(term_doc_test)\n",
    "accuracy_test = (y_test == y_pred).mean()\n",
    "accuracy_train, accuracy_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.90251999999999999, 0.87387999999999999)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# now with regularized logistic regression\n",
    "model = LogisticRegression(C=0.01, dual=True)\n",
    "model.fit(term_doc_train, y_train)\n",
    "# accuracy on training\n",
    "y_pred = model.predict(term_doc_train)\n",
    "accuracy_train = (y_train == y_pred).mean()\n",
    "# accuracy on validation\n",
    "y_pred = model.predict(term_doc_test)\n",
    "accuracy_test = (y_test == y_pred).mean()\n",
    "accuracy_train, accuracy_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nclass NBLR(keras.Model):\\n    def __init__(self):\\n        super(NBLR, self).__init__(name='NBLR')\\n        self.softmax = keras.layers.Activation('softmax')\\n\\n    def call(self, inputs):\\n        out = self.softmax(inputs)\\n        return out\\n\\nmodel = NBLR()\\nmodel.compile(loss='mean_squared_error', optimizer='sgd', metrics=['accuracy'])\\nlosses = model.fit(x=term_doc_train, y=y_train)\\n\""
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# now combining Naive Base and Logistic Regression\n",
    "\"\"\"\n",
    "class NBLR(keras.Model):\n",
    "    def __init__(self):\n",
    "        super(NBLR, self).__init__(name='NBLR')\n",
    "        self.softmax = keras.layers.Activation('softmax')\n",
    "\n",
    "    def call(self, inputs):\n",
    "        out = self.softmax(inputs)\n",
    "        return out\n",
    "\n",
    "model = NBLR()\n",
    "model.compile(loss='mean_squared_error', optimizer='sgd', metrics=['accuracy'])\n",
    "losses = model.fit(x=term_doc_train, y=y_train)\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
