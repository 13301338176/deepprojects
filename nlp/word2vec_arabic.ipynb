{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "word2vec-arabic.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "w7-6VEWddPVh",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Word2Vec on Arabic\n",
        "Implement Word2Vec in pytorch to compute vector representations of Arabic words. \n",
        "This example uses Arabic book reviews dataset - [link](http://www.mohamedaly.info/datasets/labr)\n",
        "\n",
        "The implementation is based on **Skip-gram Model** as described in the following [paper](https://arxiv.org/abs/1301.3781):\n",
        "\n",
        "    Tomas Mikolov, Kai Chen, Greg Corrado, Jeffrey Dean, Efficient Estimation of Word Representations in Vector Space, 2013\n",
        "\n",
        "<table class=\"tfo-notebook-buttons\" align=\"left\"><td>\n",
        "<a target=\"_blank\"  href=\"https://colab.research.google.com/github/dzlab/deepprojects/blob/master/nlp/word2vec_arabic.ipynb\">\n",
        "    <img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\" />Run in Google Colab</a>  \n",
        "</td><td>\n",
        "<a target=\"_blank\"  href=\"https://github.com/dzlab/deepprojects/blob/master/nlp/word2vec_arabic.ipynb\"><img width=32px src=\"https://www.tensorflow.org/images/GitHub-Mark-32px.png\" />View source on GitHub</a></td></table>"
      ]
    },
    {
      "metadata": {
        "id": "OXN1kQTigCOR",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "%matplotlib inline\n",
        "%reload_ext autoreload\n",
        "%autoreload 2"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "02FRwYHbeFFL",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Imports"
      ]
    },
    {
      "metadata": {
        "id": "Q9PaDU9F3Sks",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "vV0jwB6idLBV",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from pathlib import Path\n",
        "from tqdm import tqdm\n",
        "import re\n",
        "import collections"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "28T69Kq10vbZ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader, TensorDataset"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "CgOVuVkyeIej",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Data"
      ]
    },
    {
      "metadata": {
        "id": "8UuSBY8ngW6T",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "PATH = Path('.')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "8_PGADZreGLp",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "eb46ed06-b8d7-4767-c5ad-553a882508d0"
      },
      "cell_type": "code",
      "source": [
        "!curl -O https://raw.githubusercontent.com/mohamedadaly/LABR/master/data/reviews.tsv"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100 38.0M  100 38.0M    0     0  17.2M      0  0:00:02  0:00:02 --:--:-- 17.2M\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "-WL_nNQVeenc",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "columns = ['rating', 'review id', 'user id', 'book id', 'review']\n",
        "df = pd.read_csv(PATH/'reviews.tsv', delimiter='\\t', names=columns)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "CwIZtol6gaj4",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "ea013756-f7b1-40ff-9c45-291533923487"
      },
      "cell_type": "code",
      "source": [
        "df.head()"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>rating</th>\n",
              "      <th>review id</th>\n",
              "      <th>user id</th>\n",
              "      <th>book id</th>\n",
              "      <th>review</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>4</td>\n",
              "      <td>338670838</td>\n",
              "      <td>7878381</td>\n",
              "      <td>13431841</td>\n",
              "      <td>\"عزازيل الذي صنعناه ،الكامن في أنفسنا\" يذكرني...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>4</td>\n",
              "      <td>39428407</td>\n",
              "      <td>1775679</td>\n",
              "      <td>3554772</td>\n",
              "      <td>من أمتع ما قرأت من روايات بلا شك. وحول الشك ت...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>4</td>\n",
              "      <td>32159373</td>\n",
              "      <td>1304410</td>\n",
              "      <td>3554772</td>\n",
              "      <td>رواية تتخذ من التاريخ ،جوًا لها اختار المؤلف ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1</td>\n",
              "      <td>442326656</td>\n",
              "      <td>11333112</td>\n",
              "      <td>3554772</td>\n",
              "      <td>إني أقدّر هذه الرواية كثيرا، لسبب مختلف عن أس...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5</td>\n",
              "      <td>46492258</td>\n",
              "      <td>580165</td>\n",
              "      <td>3554772</td>\n",
              "      <td>الكاهن الذي أطلق على نفسه اسم هيبا تيمنا بالع...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   rating  review id   user id   book id  \\\n",
              "0       4  338670838   7878381  13431841   \n",
              "1       4   39428407   1775679   3554772   \n",
              "2       4   32159373   1304410   3554772   \n",
              "3       1  442326656  11333112   3554772   \n",
              "4       5   46492258    580165   3554772   \n",
              "\n",
              "                                              review  \n",
              "0   \"عزازيل الذي صنعناه ،الكامن في أنفسنا\" يذكرني...  \n",
              "1   من أمتع ما قرأت من روايات بلا شك. وحول الشك ت...  \n",
              "2   رواية تتخذ من التاريخ ،جوًا لها اختار المؤلف ...  \n",
              "3   إني أقدّر هذه الرواية كثيرا، لسبب مختلف عن أس...  \n",
              "4   الكاهن الذي أطلق على نفسه اسم هيبا تيمنا بالع...  "
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "metadata": {
        "id": "3DisB55NgiDN",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "look at the lengths of review sentences"
      ]
    },
    {
      "metadata": {
        "id": "OaOO_ogwgcIS",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "review_lengths = df['review'].apply(lambda x: len(x.split())).tolist()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "1kBM0pDGgeqW",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 389
        },
        "outputId": "ce0a9eb2-67bb-44b6-9a35-6c69e20c3d35"
      },
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(8,6))\n",
        "plt.hist(review_lengths, bins='auto', range=[0, 200])\n",
        "plt.title(\"Review lengths (in words count)\")\n",
        "plt.show()"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAeoAAAF0CAYAAADsAXoJAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nTkSuQmCC\n",
            "text/plain": [
              "<Figure size 576x432 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "brrqxixxgnIQ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Processing\n",
        "Simple Text tokenization and cleaning"
      ]
    },
    {
      "metadata": {
        "id": "be5D__LahXUp",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "char1_re = re.compile('\"')\n",
        "char2_re = re.compile('،')\n",
        "char3_re = re.compile('\\.')\n",
        "char4_re = re.compile('!')\n",
        "char5_re = re.compile('؟')\n",
        "char6_re = re.compile('؛')\n",
        "char7_re = re.compile('-')\n",
        "char8_re = re.compile('\\(')\n",
        "char9_re = re.compile('\\)')\n",
        "\n",
        "def cleanup(text):\n",
        "    text = char1_re.sub(' \" ', text)\n",
        "    text = char2_re.sub(' ، ', text)\n",
        "    text = char3_re.sub(' . ', text)\n",
        "    text = char4_re.sub(' ! ', text)\n",
        "    text = char5_re.sub(' ؟ ', text)\n",
        "    text = char6_re.sub(' ؛ ', text)\n",
        "    text = char7_re.sub(' - ', text)\n",
        "    text = char8_re.sub(' ( ', text)\n",
        "    text = char9_re.sub(' ) ', text)\n",
        "    return text"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "iQKhMJqFgweO",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "add special tokens"
      ]
    },
    {
      "metadata": {
        "id": "HYl1R4yOgkdb",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "BOS = '_BOS_' # Begining of sentence token\n",
        "EOS = '_EOS_' # Ending of sentence token\n",
        "PAD = '_PAD_' # Padding token\n",
        "UNK = '_UNK_' # Unknown tokens (for unknown and low frequency words)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "dMduZhr3g3i3",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "22e5dbc0-d2de-40ee-d5b9-b2a9bbb59429"
      },
      "cell_type": "code",
      "source": [
        "all_text = f' {PAD} '.join([f'{BOS} {review} {EOS}' for review in tqdm(df['review'].tolist())])"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 63257/63257 [00:00<00:00, 904729.24it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "sz-oo7hNjrlc",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "283d4b94-779a-4966-85f2-9b9fb3799f48"
      },
      "cell_type": "code",
      "source": [
        "cleanup(all_text[:1_000])"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'_BOS_   \" عزازيل الذي صنعناه  ، الكامن في أنفسنا \"  يذكرني يوسف زيدان بــ بورخس في استخدامه لحيلته الفنية ، وخداع القاريء بأن الرواية ترجمة لمخطوط قديم .  الهوامش المخترعة و اختلاق وجود مترجـِم عاد بي إلى بورخس و هوامشه و كتَّابه الوهميين .  هذه أولى قراءاتي ليوسف زيدان  ، وهو عبقري في السرد ويخلقُ جوَّا ساحرا متفرداً يغرقك في المتعة .  هُنا يتجلى الشكُّ الراقي الممزوج بانسانية هيبا الفاتنة ربما تم تناول فكرة الرواية قبلاً  ، ولكن هنا تفرداً و عذوبة لا تُقارن بنصٍ آخر كنتُ أودُّ لو صيغت النهاية بطريقة مختلفة فقد جاءت باردة لا تتناسب مع رواية خُطَّت بهذا الشغف  .  ولذا لا أستطيع منح الرواية خمس نجوم  ، وإن كانت تجربة قرائية متفردة وممتعة .   _EOS_ _PAD_ _BOS_  من أمتع ما قرأت من روايات بلا شك .  وحول الشك تدندن  ( عزازيل )  بلا هوادة .  أحمد الديب 2008 _EOS_ _PAD_ _BOS_  رواية تتخذ من التاريخ  ، جوًا لها اختار المؤلف فترة تاريخية ندر من يتناولها روائيًا .  مكتوبة بدقة وإتقان وجمال . من أروع ما يمكن أن تقرأ من الروايات التاريخية .  تركز على الإنسان . صانع المعنى ومدمره  .  _EOS_ _PAD_ _BOS_  إني أقدّر هذه الرواية كثيرا ،  لسبب مختلف'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "metadata": {
        "id": "IGYopft6g_KU",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "all_text = cleanup(all_text)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "sgz9zWVHq70E",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#### Word2Vec Parameters"
      ]
    },
    {
      "metadata": {
        "id": "WuAXIPLOhBY7",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# input parameters\n",
        "occur_min   = 10    # Remove all words that does not appears at least n times\n",
        "num_skips   = 2     # How many times to reuse an input to generate a label\n",
        "num_sampled = 64    # Number of negative examples to sample\n",
        "# model parameters\n",
        "vocab_size  = 10000 # Number of different words in the vocabulary (V)\n",
        "embed_size  = 200   # Dimension of the word representations (D)\n",
        "skip_window = 3     # Number of words considered left and right (C)\n",
        "\n",
        "batch_size  = 64"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ktpUUWzPsTdY",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "words = [w for w in all_text.split(' ') if w is not '']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "f3vnqziYtUOy",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#### Filter out words with low frequency to limit vocab size"
      ]
    },
    {
      "metadata": {
        "id": "FgHVSlK6rOR2",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "b04b1f9f-2dd6-4393-8357-074e833246fc"
      },
      "cell_type": "code",
      "source": [
        "counts = collections.defaultdict(lambda: 0)\n",
        "\n",
        "for w in tqdm(words):\n",
        "    if w is '': continue\n",
        "    counts[w] = counts[w] + 1"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 4383439/4383439 [00:02<00:00, 1475491.64it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "i8Welx7asa0t",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "d08beb27-2f71-4fee-c8fe-a23d103b1231"
      },
      "cell_type": "code",
      "source": [
        "counts['تجربة'], counts['رواية']"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(879, 10310)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "metadata": {
        "id": "XjQfqP7shufg",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "limit size of vocabulary"
      ]
    },
    {
      "metadata": {
        "id": "8lkS8yZXhuMo",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "counts = dict(list(sorted(counts.items(), reverse=True, key=lambda x: x[1]))[:vocab_size])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "u9QjBEhot9tq",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "words with lowest frequencies"
      ]
    },
    {
      "metadata": {
        "id": "CJXGss6-t711",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "ordered_counts = dict(sorted(counts.items(), reverse=True, key=lambda x: x[1]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "LV0DCNzuuihY",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        },
        "outputId": "a6e942e1-d65b-433c-a96b-9f6598808e05"
      },
      "cell_type": "code",
      "source": [
        "list(ordered_counts.items())[-10:]"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('العمامة', 10),\n",
              " ('عنواني', 10),\n",
              " ('الفئران', 10),\n",
              " ('فرونسكي', 10),\n",
              " ('كارنين', 10),\n",
              " ('عيدي', 10),\n",
              " ('حسيني', 10),\n",
              " ('يثوروا', 10),\n",
              " ('غلاظة', 10),\n",
              " ('السيّاب:', 10)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 37
        }
      ]
    },
    {
      "metadata": {
        "id": "nKAhdzQKte49",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "vocab = [w for w, c in counts.items() if c >= occur_min]\n",
        "vocab_size = len(vocab)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "7fLYfCg0vY7l",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Create indices on vocabulary"
      ]
    },
    {
      "metadata": {
        "id": "2ccir2A2u-2I",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "itoa = dict((i, w) for i, w in enumerate(vocab))\n",
        "atoi = dict((w, i) for i, w in enumerate(vocab))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "jvH2FkzRvAcI",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "bc7cb532-ffe9-4bd8-ef92-d26ad983ec93"
      },
      "cell_type": "code",
      "source": [
        "atoi['تجربة'], atoi['رواية']"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(72, 62)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "metadata": {
        "id": "VhncbknvwnEP",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#### Add the unknown token"
      ]
    },
    {
      "metadata": {
        "id": "zGpfK0WtwqWB",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "atoi[UNK] = len(vocab)\n",
        "itoa[len(vocab)] = UNK"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "gKVtNhBov8G9",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "atoi = collections.defaultdict(lambda: len(vocab), atoi)\n",
        "itoa = collections.defaultdict(lambda: UNK, itoa)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "SZCNex2qxHDk",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "cf7f0d91-d851-4a79-b2c4-5aab737bd3a1"
      },
      "cell_type": "code",
      "source": [
        "atoi['not found'], itoa[len(vocab) * 2]"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(32057, '_UNK_')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "metadata": {
        "id": "v8VIBBFb0VDf",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Training Input"
      ]
    },
    {
      "metadata": {
        "id": "e238oa3ZHy-l",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "919bdb1c-625e-446a-e5c6-807bf1c225d3"
      },
      "cell_type": "code",
      "source": [
        "# bounding indices\n",
        "min_index = 0\n",
        "max_index = len(words)-1\n",
        "# the list of ngrams that will be \n",
        "X, y = [], []\n",
        "# create dataset\n",
        "for t, wt in tqdm(enumerate(words)):\n",
        "    # add words from the previous skip window\n",
        "    for past in range(t-1, max(min_index, t-skip_window)):\n",
        "        X.append(atoi[wt])\n",
        "        y.append(atoi[words[past]])\n",
        "    # add words from the next skip window\n",
        "    for future in range(t+1, min(max_index, t+skip_window)):\n",
        "        X.append(atoi[wt])\n",
        "        y.append(atoi[words[future]])"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "4383439it [00:13, 324124.03it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "zcnwBdowWQS9",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "split input into training and validation sets"
      ]
    },
    {
      "metadata": {
        "id": "H8VEKwlvfZL5",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "split_index = int(len(X) * 0.9)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "hJzCzNbXRvm9",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "X_train, y_train = torch.LongTensor(X[:split_index]).cuda(), torch.LongTensor(y[:split_index]).cuda()\n",
        "\n",
        "train_ds = TensorDataset(X_train, y_train)\n",
        "train_dl = DataLoader(train_ds, batch_size=batch_size, shuffle=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "7ImyJX_GJX5w",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        },
        "outputId": "46544d93-fca3-4aaa-a32e-4d0f16001e61"
      },
      "cell_type": "code",
      "source": [
        "#i = 0\n",
        "#for win, wout in ngrams:\n",
        "#    print(win, wout)\n",
        "#    i += 1\n",
        "#    if i==10: break"
      ],
      "execution_count": 144,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "_BOS_ _EOS_\n",
            "_BOS_ \"\n",
            "_BOS_ عزازيل\n",
            "\" عزازيل\n",
            "\" الذي\n",
            "عزازيل الذي\n",
            "عزازيل صنعناه\n",
            "الذي صنعناه\n",
            "الذي ،\n",
            "صنعناه ،\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "3QDQc01x91Ay",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class SkipGramDataset(Dataset):\n",
        "    \"\"\"Skip Gram dataset.\"\"\"\n",
        "\n",
        "    def __init__(self, vocab, atoi, itoa, ngrams):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            vocab      (list): list of words constinuting the Vocabulary.\n",
        "            atoi (dictionary): word to index Directory.\n",
        "            itoa (dictionary): index to word Directory.\n",
        "            ngrams     (list): list of ngrams created with a skip window (left & right) \n",
        "        \"\"\"\n",
        "        self.vocab = vocab\n",
        "        self.atoi, self.itoa = atoi, itoa\n",
        "        self.ngrams = ngrams\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.ngrams)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        win, wout = ngrams[idx]\n",
        "        return (self.atoi(win), self.atoi(wout))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "kJZ8Xki6xOu-",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#dataset = SkipGramDataset(vocab, atoi, itoa, ngrams)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "asfrWhb_0cN8",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Model\n",
        "Continuous Skip-gram Model in PyTorch"
      ]
    },
    {
      "metadata": {
        "id": "C-KsVZgm0gEi",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class SkipGramModel(nn.Module):\n",
        "    def __init__(self, vocab_size, embed_size, skip_window):\n",
        "        super().__init__()\n",
        "        self.vocab_size, self.embed_size = vocab_size, embed_size\n",
        "        self.embeds = nn.Embedding(num_embeddings=vocab_size, embedding_dim=embed_size)\n",
        "        self.lin1 = nn.Linear(in_features=embed_size, out_features=vocab_size)\n",
        "        self.lin2 = nn.Linear(in_features=vocab_size, out_features=vocab_size)\n",
        "        \n",
        "    def forward(self, X):\n",
        "        # X shape should be (batch_size, 1)\n",
        "        y = self.embeds(X)                     # (batch_size, embedding_dim)\n",
        "        y = F.relu(self.lin1(y))               # (batch_size, vocab_size)\n",
        "        y = F.log_softmax(self.lin2(y), dim=1) # (batch_size, vocab_size)\n",
        "        return y"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "MBF_ll5OYCCq",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "model = SkipGramModel(vocab_size, embed_size, skip_window).cuda()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "fKkDWhI8LHA5",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "loss_function = nn.NLLLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.01)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "1MViCTpnLkww",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Train"
      ]
    },
    {
      "metadata": {
        "id": "coMtraW0No6r",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def run(model, dataloader):\n",
        "    total_loss = 0\n",
        "    # for every batch in the DataLoader\n",
        "    for batch in tqdm(dataloader):\n",
        "        # Step 1. Prepare the inputs to be passed to the model (i.e, turn the words\n",
        "        # into integer indices and wrap them in tensors)\n",
        "        X, y = batch\n",
        "        # Step 2. Recall that torch *accumulates* gradients. Before passing in a\n",
        "        # new instance, you need to zero out the gradients from the old\n",
        "        # instance\n",
        "        model.zero_grad()\n",
        "\n",
        "        # Step 3. Run the forward pass, getting log probabilities over next words\n",
        "        log_probs = model(X)\n",
        "        \n",
        "        # Step 4. Compute your loss function. (Again, Torch wants the target\n",
        "        # word wrapped in a tensor)\n",
        "        #loss = loss_function(log_probs, torch.tensor([word_to_ix[target]], dtype=torch.long))\n",
        "        loss = loss_function(log_probs, y)\n",
        "        \n",
        "        # Step 5. Do the backward pass and update the gradient\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # Get the Python number from a 1-element Tensor by calling tensor.item()\n",
        "        total_loss += loss.item()\n",
        "        \n",
        "    return total_loss"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "NOpkJIN-LeDv",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 963
        },
        "outputId": "99389e60-534b-43d9-947b-0f498acda4b9"
      },
      "cell_type": "code",
      "source": [
        "losses = []\n",
        "for epoch in range(10):\n",
        "    total_loss = run(model, train_dl)\n",
        "    losses.append(total_loss)\n",
        "# Check if the loss decreased after every iteration\n",
        "print(losses)  "
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\r  0%|          | 0/123285 [00:00<?, ?it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-27-42e434c1a945>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mlosses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mtotal_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_dl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mlosses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtotal_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# Check if the loss decreased after every iteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-26-c1eac4b9679f>\u001b[0m in \u001b[0;36mrun\u001b[0;34m(model, dataloader)\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0;31m# Step 3. Run the forward pass, getting log probabilities over next words\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m         \u001b[0mlog_probs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0;31m# Step 4. Compute your loss function. (Again, Torch wants the target\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    487\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    488\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 489\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    490\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    491\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-23-f7923dba52f5>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0;31m# X shape should be (batch_size, 1)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membeds\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m                     \u001b[0;31m# (batch_size, embedding_dim)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m         \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlin1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m               \u001b[0;31m# (batch_size, vocab_size)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m         \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog_softmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlin2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# (batch_size, vocab_size)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    487\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    488\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 489\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    490\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    491\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mweak_script_method\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mlinear\u001b[0;34m(input, weight, bias)\u001b[0m\n\u001b[1;32m   1350\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mbias\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1351\u001b[0m         \u001b[0;31m# fused op is marginally faster\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1352\u001b[0;31m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddmm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_unwrap_optional\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1353\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1354\u001b[0m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: cublas runtime error : resource allocation failed at /pytorch/aten/src/THC/THCGeneral.cpp:250"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "dvmLPWavfTRY",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Validation"
      ]
    },
    {
      "metadata": {
        "id": "Gn_h0qozbHUI",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "X_valid, y_valid = torch.LongTensor(X[split_index:]).cuda(), torch.LongTensor(y[split_index:]).cuda()\n",
        "\n",
        "valid_ds = TensorDataset(X_valid, y_valid)\n",
        "valid_dl = DataLoader(valid_ds, batch_size=batch_size, shuffle=True)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}