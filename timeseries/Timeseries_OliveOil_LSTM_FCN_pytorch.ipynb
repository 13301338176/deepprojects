{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Timeseries - OliveOil - LSTM-FCN-pytorch.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "NreY7BzeqHVd",
        "colab_type": "code",
        "outputId": "e1b9c7d2-9a41-4189-c0d0-2e225be696a7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1397
        }
      },
      "cell_type": "code",
      "source": [
        "!curl https://course-v3.fast.ai/setup/colab | bash"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "\r  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\r  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\r100   665  100   665    0     0   1973      0 --:--:-- --:--:-- --:--:--  1973\n",
            "Collecting pillow==4.1.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/36/e5/88b3d60924a3f8476fa74ec086f5fbaba56dd6cee0d82845f883b6b6dd18/Pillow-4.1.1-cp36-cp36m-manylinux1_x86_64.whl (5.7MB)\n",
            "\u001b[K    100% |████████████████████████████████| 5.7MB 7.4MB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: olefile in /usr/local/lib/python3.6/dist-packages (from pillow==4.1.1) (0.46)\n",
            "Installing collected packages: pillow\n",
            "  Found existing installation: Pillow 4.0.0\n",
            "    Uninstalling Pillow-4.0.0:\n",
            "      Successfully uninstalled Pillow-4.0.0\n",
            "Successfully installed pillow-4.1.1\n",
            "Looking in links: https://download.pytorch.org/whl/nightly/cu92/torch_nightly.html\n",
            "Collecting torch_nightly\n",
            "\u001b[?25l  Downloading https://download.pytorch.org/whl/nightly/cu92/torch_nightly-1.0.0.dev20181128-cp36-cp36m-linux_x86_64.whl (576.6MB)\n",
            "\u001b[K    100% |████████████████████████████████| 576.6MB 23kB/s \n",
            "tcmalloc: large alloc 1073750016 bytes == 0x62d26000 @  0x7fb3be9692a4 0x591a07 0x5b5d56 0x502e9a 0x506859 0x502209 0x502f3d 0x506859 0x504c28 0x502540 0x502f3d 0x506859 0x504c28 0x502540 0x502f3d 0x506859 0x504c28 0x502540 0x502f3d 0x507641 0x502209 0x502f3d 0x506859 0x504c28 0x502540 0x502f3d 0x507641 0x504c28 0x502540 0x502f3d 0x507641\n",
            "\u001b[?25hInstalling collected packages: torch-nightly\n",
            "Successfully installed torch-nightly-1.0.0.dev20181128\n",
            "Cloning into 'course-v3'...\n",
            "remote: Enumerating objects: 2501, done.\u001b[K\n",
            "remote: Total 2501 (delta 0), reused 0 (delta 0), pack-reused 2501\u001b[K\n",
            "Receiving objects: 100% (2501/2501), 69.50 MiB | 27.18 MiB/s, done.\n",
            "Resolving deltas: 100% (1361/1361), done.\n",
            "Collecting fastai\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/84/13/dfb1683762bc2e2ee0503e08e812a164f710a8b919c050fd5838c150f3ca/fastai-1.0.29-py3-none-any.whl (131kB)\n",
            "\u001b[K    100% |████████████████████████████████| 133kB 3.9MB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: typing in /usr/local/lib/python3.6/dist-packages (from fastai) (3.6.6)\n",
            "Requirement already satisfied, skipping upgrade: numpy>=1.12 in /usr/local/lib/python3.6/dist-packages (from fastai) (1.14.6)\n",
            "Requirement already satisfied, skipping upgrade: spacy==2.0.16 in /usr/local/lib/python3.6/dist-packages (from fastai) (2.0.16)\n",
            "Requirement already satisfied, skipping upgrade: thinc==6.12.0 in /usr/local/lib/python3.6/dist-packages (from fastai) (6.12.0)\n",
            "Collecting bottleneck (from fastai)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/05/ae/cedf5323f398ab4e4ff92d6c431a3e1c6a186f9b41ab3e8258dff786a290/Bottleneck-1.2.1.tar.gz (105kB)\n",
            "\u001b[K    100% |████████████████████████████████| 112kB 5.2MB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: regex in /usr/local/lib/python3.6/dist-packages (from fastai) (2018.1.10)\n",
            "Collecting numexpr (from fastai)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/db/ea/efd9e16283637eb5b6c0042b6cc3521f1b9a5b47767ac463c88bbd37670c/numexpr-2.6.8-cp36-cp36m-manylinux1_x86_64.whl (162kB)\n",
            "\u001b[K    100% |████████████████████████████████| 163kB 5.8MB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: requests in /usr/local/lib/python3.6/dist-packages (from fastai) (2.18.4)\n",
            "Requirement already satisfied, skipping upgrade: scipy in /usr/local/lib/python3.6/dist-packages (from fastai) (1.1.0)\n",
            "Collecting torchvision-nightly (from fastai)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ca/bd/d0f9a33c81c79710eb7ee428b66869b49a8be16c7f1e446c211a7fbfb7be/torchvision_nightly-0.2.1-py2.py3-none-any.whl (54kB)\n",
            "\u001b[K    100% |████████████████████████████████| 61kB 11.4MB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: pyyaml in /usr/local/lib/python3.6/dist-packages (from fastai) (3.13)\n",
            "Collecting dataclasses (from fastai)\n",
            "  Downloading https://files.pythonhosted.org/packages/26/2f/1095cdc2868052dd1e64520f7c0d5c8c550ad297e944e641dbf1ffbb9a5d/dataclasses-0.6-py3-none-any.whl\n",
            "Requirement already satisfied, skipping upgrade: pandas in /usr/local/lib/python3.6/dist-packages (from fastai) (0.22.0)\n",
            "Requirement already satisfied, skipping upgrade: matplotlib in /usr/local/lib/python3.6/dist-packages (from fastai) (2.1.2)\n",
            "Requirement already satisfied, skipping upgrade: cymem==2.0.2 in /usr/local/lib/python3.6/dist-packages (from fastai) (2.0.2)\n",
            "Requirement already satisfied, skipping upgrade: Pillow in /usr/local/lib/python3.6/dist-packages (from fastai) (4.1.1)\n",
            "Collecting fastprogress>=0.1.16 (from fastai)\n",
            "  Downloading https://files.pythonhosted.org/packages/ea/93/b35cabbab4d25a2fdc5cd196114fbe1160451df5cf1459a80781893f3b0f/fastprogress-0.1.16-py3-none-any.whl\n",
            "Requirement already satisfied, skipping upgrade: ujson>=1.35 in /usr/local/lib/python3.6/dist-packages (from spacy==2.0.16->fastai) (1.35)\n",
            "Requirement already satisfied, skipping upgrade: msgpack-numpy<0.4.4 in /usr/local/lib/python3.6/dist-packages (from spacy==2.0.16->fastai) (0.4.3.2)\n",
            "Requirement already satisfied, skipping upgrade: preshed<2.1.0,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from spacy==2.0.16->fastai) (2.0.1)\n",
            "Requirement already satisfied, skipping upgrade: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.6/dist-packages (from spacy==2.0.16->fastai) (1.0.1)\n",
            "Requirement already satisfied, skipping upgrade: dill<0.3,>=0.2 in /usr/local/lib/python3.6/dist-packages (from spacy==2.0.16->fastai) (0.2.8.2)\n",
            "Requirement already satisfied, skipping upgrade: plac<1.0.0,>=0.9.6 in /usr/local/lib/python3.6/dist-packages (from spacy==2.0.16->fastai) (0.9.6)\n",
            "Requirement already satisfied, skipping upgrade: cytoolz<0.10,>=0.9.0 in /usr/local/lib/python3.6/dist-packages (from thinc==6.12.0->fastai) (0.9.0.1)\n",
            "Requirement already satisfied, skipping upgrade: tqdm<5.0.0,>=4.10.0 in /usr/local/lib/python3.6/dist-packages (from thinc==6.12.0->fastai) (4.28.1)\n",
            "Requirement already satisfied, skipping upgrade: wrapt<1.11.0,>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from thinc==6.12.0->fastai) (1.10.11)\n",
            "Requirement already satisfied, skipping upgrade: six<2.0.0,>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from thinc==6.12.0->fastai) (1.11.0)\n",
            "Requirement already satisfied, skipping upgrade: msgpack<1.0.0,>=0.5.6 in /usr/local/lib/python3.6/dist-packages (from thinc==6.12.0->fastai) (0.5.6)\n",
            "Requirement already satisfied, skipping upgrade: idna<2.7,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->fastai) (2.6)\n",
            "Requirement already satisfied, skipping upgrade: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->fastai) (3.0.4)\n",
            "Requirement already satisfied, skipping upgrade: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->fastai) (2018.10.15)\n",
            "Requirement already satisfied, skipping upgrade: urllib3<1.23,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->fastai) (1.22)\n",
            "Requirement already satisfied, skipping upgrade: pytz>=2011k in /usr/local/lib/python3.6/dist-packages (from pandas->fastai) (2018.7)\n",
            "Requirement already satisfied, skipping upgrade: python-dateutil>=2 in /usr/local/lib/python3.6/dist-packages (from pandas->fastai) (2.5.3)\n",
            "Requirement already satisfied, skipping upgrade: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib->fastai) (0.10.0)\n",
            "Requirement already satisfied, skipping upgrade: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->fastai) (2.3.0)\n",
            "Requirement already satisfied, skipping upgrade: olefile in /usr/local/lib/python3.6/dist-packages (from Pillow->fastai) (0.46)\n",
            "Requirement already satisfied, skipping upgrade: toolz>=0.8.0 in /usr/local/lib/python3.6/dist-packages (from cytoolz<0.10,>=0.9.0->thinc==6.12.0->fastai) (0.9.0)\n",
            "Building wheels for collected packages: bottleneck\n",
            "  Running setup.py bdist_wheel for bottleneck ... \u001b[?25l-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \bdone\n",
            "\u001b[?25h  Stored in directory: /root/.cache/pip/wheels/f2/bf/ec/e0f39aa27001525ad455139ee57ec7d0776fe074dfd78c97e4\n",
            "Successfully built bottleneck\n",
            "Installing collected packages: bottleneck, numexpr, torchvision-nightly, dataclasses, fastprogress, fastai\n",
            "Successfully installed bottleneck-1.2.1 dataclasses-0.6 fastai-1.0.29 fastprogress-0.1.16 numexpr-2.6.8 torchvision-nightly-0.2.1\n",
            "Already up to date.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "CKP2ikW1ngNl",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "%reload_ext autoreload\n",
        "%autoreload 2\n",
        "%matplotlib inline"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "OblomiW9n4bg",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import pathlib\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from tqdm import tqdm\n",
        "\n",
        "from fastai import *"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "pEwnf-gdntsa",
        "colab_type": "code",
        "outputId": "755e3dd5-003e-4cd3-d8bd-ec5a350fd533",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "cell_type": "code",
      "source": [
        "!curl -O http://www.timeseriesclassification.com/Downloads/OliveOil.zip"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100  345k  100  345k    0     0   336k      0  0:00:01  0:00:01 --:--:--  336k\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "Qsr9wx88nyRG",
        "colab_type": "code",
        "outputId": "601f3ddb-f72f-476c-d846-757a39cdda7b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        }
      },
      "cell_type": "code",
      "source": [
        "!unzip OliveOil.zip"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Archive:  OliveOil.zip\n",
            "  inflating: OliveOil.txt            \n",
            "  inflating: OliveOil_TEST.arff      \n",
            "  inflating: OliveOil_TEST.txt       \n",
            "  inflating: OliveOil_TRAIN.arff     \n",
            "  inflating: OliveOil_TRAIN.txt      \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "dg9VZU3nqqOv",
        "colab_type": "code",
        "outputId": "55368e61-3e25-4387-84cc-2128970bdb49",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 253
        }
      },
      "cell_type": "code",
      "source": [
        "# read a text file of timeseries data into a pandas DataFrame\n",
        "def txt2df(fname):\n",
        "    raw = open(fname).readlines()\n",
        "    raw = [map(float, each.strip().split()) for each in raw]\n",
        "    df_data = list()\n",
        "    for i in range(len(raw)):\n",
        "        df_data.append(list(raw[i]))\n",
        "    # column '0' contains the labels\n",
        "    result = pd.DataFrame(df_data)\n",
        "    result.astype({0: int})\n",
        "    return result\n",
        "\n",
        "df = txt2df('OliveOil_TRAIN.txt'); df.head()"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>...</th>\n",
              "      <th>561</th>\n",
              "      <th>562</th>\n",
              "      <th>563</th>\n",
              "      <th>564</th>\n",
              "      <th>565</th>\n",
              "      <th>566</th>\n",
              "      <th>567</th>\n",
              "      <th>568</th>\n",
              "      <th>569</th>\n",
              "      <th>570</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1.0</td>\n",
              "      <td>-0.611375</td>\n",
              "      <td>-0.610586</td>\n",
              "      <td>-0.606557</td>\n",
              "      <td>-0.601132</td>\n",
              "      <td>-0.594315</td>\n",
              "      <td>-0.585762</td>\n",
              "      <td>-0.577419</td>\n",
              "      <td>-0.570175</td>\n",
              "      <td>-0.563285</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.979553</td>\n",
              "      <td>-0.980385</td>\n",
              "      <td>-0.980328</td>\n",
              "      <td>-0.980220</td>\n",
              "      <td>-0.980691</td>\n",
              "      <td>-0.981337</td>\n",
              "      <td>-0.982336</td>\n",
              "      <td>-0.983213</td>\n",
              "      <td>-0.983672</td>\n",
              "      <td>-0.983120</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1.0</td>\n",
              "      <td>-0.615392</td>\n",
              "      <td>-0.613729</td>\n",
              "      <td>-0.609228</td>\n",
              "      <td>-0.604315</td>\n",
              "      <td>-0.598768</td>\n",
              "      <td>-0.590507</td>\n",
              "      <td>-0.581617</td>\n",
              "      <td>-0.572926</td>\n",
              "      <td>-0.565374</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.979210</td>\n",
              "      <td>-0.979335</td>\n",
              "      <td>-0.979523</td>\n",
              "      <td>-0.980038</td>\n",
              "      <td>-0.981146</td>\n",
              "      <td>-0.982332</td>\n",
              "      <td>-0.982570</td>\n",
              "      <td>-0.982630</td>\n",
              "      <td>-0.982850</td>\n",
              "      <td>-0.982549</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1.0</td>\n",
              "      <td>-0.611999</td>\n",
              "      <td>-0.610500</td>\n",
              "      <td>-0.606374</td>\n",
              "      <td>-0.600445</td>\n",
              "      <td>-0.593084</td>\n",
              "      <td>-0.585245</td>\n",
              "      <td>-0.577118</td>\n",
              "      <td>-0.568827</td>\n",
              "      <td>-0.561596</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.979514</td>\n",
              "      <td>-0.979663</td>\n",
              "      <td>-0.979964</td>\n",
              "      <td>-0.980563</td>\n",
              "      <td>-0.981326</td>\n",
              "      <td>-0.982744</td>\n",
              "      <td>-0.983107</td>\n",
              "      <td>-0.983033</td>\n",
              "      <td>-0.983956</td>\n",
              "      <td>-0.983858</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1.0</td>\n",
              "      <td>-0.622784</td>\n",
              "      <td>-0.622222</td>\n",
              "      <td>-0.619049</td>\n",
              "      <td>-0.613251</td>\n",
              "      <td>-0.605889</td>\n",
              "      <td>-0.597508</td>\n",
              "      <td>-0.589047</td>\n",
              "      <td>-0.580697</td>\n",
              "      <td>-0.572822</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.968819</td>\n",
              "      <td>-0.969902</td>\n",
              "      <td>-0.970586</td>\n",
              "      <td>-0.970718</td>\n",
              "      <td>-0.971197</td>\n",
              "      <td>-0.972268</td>\n",
              "      <td>-0.972654</td>\n",
              "      <td>-0.972861</td>\n",
              "      <td>-0.973671</td>\n",
              "      <td>-0.973614</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1.0</td>\n",
              "      <td>-0.621793</td>\n",
              "      <td>-0.621272</td>\n",
              "      <td>-0.617298</td>\n",
              "      <td>-0.612074</td>\n",
              "      <td>-0.605472</td>\n",
              "      <td>-0.597607</td>\n",
              "      <td>-0.589165</td>\n",
              "      <td>-0.581424</td>\n",
              "      <td>-0.574620</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.977420</td>\n",
              "      <td>-0.977831</td>\n",
              "      <td>-0.978376</td>\n",
              "      <td>-0.978778</td>\n",
              "      <td>-0.979656</td>\n",
              "      <td>-0.981148</td>\n",
              "      <td>-0.981796</td>\n",
              "      <td>-0.981331</td>\n",
              "      <td>-0.981289</td>\n",
              "      <td>-0.981331</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 571 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "   0         1         2         3         4         5         6         7    \\\n",
              "0  1.0 -0.611375 -0.610586 -0.606557 -0.601132 -0.594315 -0.585762 -0.577419   \n",
              "1  1.0 -0.615392 -0.613729 -0.609228 -0.604315 -0.598768 -0.590507 -0.581617   \n",
              "2  1.0 -0.611999 -0.610500 -0.606374 -0.600445 -0.593084 -0.585245 -0.577118   \n",
              "3  1.0 -0.622784 -0.622222 -0.619049 -0.613251 -0.605889 -0.597508 -0.589047   \n",
              "4  1.0 -0.621793 -0.621272 -0.617298 -0.612074 -0.605472 -0.597607 -0.589165   \n",
              "\n",
              "        8         9      ...          561       562       563       564  \\\n",
              "0 -0.570175 -0.563285    ...    -0.979553 -0.980385 -0.980328 -0.980220   \n",
              "1 -0.572926 -0.565374    ...    -0.979210 -0.979335 -0.979523 -0.980038   \n",
              "2 -0.568827 -0.561596    ...    -0.979514 -0.979663 -0.979964 -0.980563   \n",
              "3 -0.580697 -0.572822    ...    -0.968819 -0.969902 -0.970586 -0.970718   \n",
              "4 -0.581424 -0.574620    ...    -0.977420 -0.977831 -0.978376 -0.978778   \n",
              "\n",
              "        565       566       567       568       569       570  \n",
              "0 -0.980691 -0.981337 -0.982336 -0.983213 -0.983672 -0.983120  \n",
              "1 -0.981146 -0.982332 -0.982570 -0.982630 -0.982850 -0.982549  \n",
              "2 -0.981326 -0.982744 -0.983107 -0.983033 -0.983956 -0.983858  \n",
              "3 -0.971197 -0.972268 -0.972654 -0.972861 -0.973671 -0.973614  \n",
              "4 -0.979656 -0.981148 -0.981796 -0.981331 -0.981289 -0.981331  \n",
              "\n",
              "[5 rows x 571 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "metadata": {
        "id": "uBxLpG8Wr3nS",
        "colab_type": "code",
        "outputId": "2b90c06d-d0e6-443c-9968-13dec10c0080",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "df[0].unique()"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([1., 2., 3., 4.])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "metadata": {
        "id": "onhUsHlhsWvc",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "bs = 64"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "O1pj6M8Vn-0D",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "DATASET = 'OliveOil'\n",
        "classes = 4"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "9EMN9yFin1cW",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "path = pathlib.Path('')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "tDncQ40aHJkX",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def one_hot_encode(input, labels):\n",
        "    m = input.shape[0]\n",
        "    output = np.zeros((m, labels), dtype=int)\n",
        "    row_index = np.arange(m)\n",
        "    output[row_index, input] = 1\n",
        "    return output\n",
        "\n",
        "def split_xy(data, classes):\n",
        "    X = data[:, 1:]\n",
        "    y = data[:, 0].astype(int)\n",
        "    # hot encode\n",
        "    #y = one_hot_encode(y, classes)\n",
        "    return X, y\n",
        "\n",
        "def create_dataset(X, y, device):\n",
        "    X_tensor = torch.tensor(X, dtype=torch.float32, device=device)\n",
        "    y_tensor = torch.tensor(y, dtype=torch.long, device=device)\n",
        "    #y_tensor = torch.tensor(y, dtype=torch.float32, device=device)\n",
        "    return TensorDataset(X_tensor, y_tensor)\n",
        "\n",
        "def load_data(path, classes):\n",
        "    data = np.loadtxt(path)\n",
        "    return split_xy(data, classes)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "jNG4E5F-Tpcf",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "The outputs of the model should be of size (minibatch, C). On the other hand the target `y` should contain the indices of the classes."
      ]
    },
    {
      "metadata": {
        "id": "A69YvILXoA5e",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# load training dataset\n",
        "X_train, y_train = load_data(path/'OliveOil_TRAIN.txt', classes) \n",
        "\n",
        "# load testing dataset\n",
        "X_test, y_test = load_data(path/'OliveOil_TEST.txt', classes)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "i4QHlPkTxzcn",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "y_train = y_train - 1\n",
        "y_test = y_test - 1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "OOrCEhRicN27",
        "colab_type": "code",
        "outputId": "b5fc62c3-0b36-492f-8b88-93ad6ecfaf77",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "cell_type": "code",
      "source": [
        "print('X_train %s   y_train %s   y_train type %s' % (X_train.shape, y_train.shape, y_train.dtype))\n",
        "print('X_test  %s   y_test  %s   y_test  type %s' % (X_test.shape, y_test.shape, y_test.dtype))"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "X_train (30, 570)   y_train (30,)   y_train type int64\n",
            "X_test  (30, 570)   y_test  (30,)   y_test  type int64\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "MWAHekuN7mqq",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "As the classes are imbalanced, get the count for each class, to use later in the sampling"
      ]
    },
    {
      "metadata": {
        "id": "tQ4U0nAa7RV3",
        "colab_type": "code",
        "outputId": "966fe4f4-76b6-4331-cd6f-3a301e1b7923",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "cell_type": "code",
      "source": [
        "for c in [1, 2, 3, 4]:\n",
        "    c = c - 1\n",
        "    class_count = (y_train==c).sum()\n",
        "    print('Class %d occured %d and percentage %f' % (c, class_count, class_count/y_train.shape[0]))"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Class 0 occured 5 and percentage 0.166667\n",
            "Class 1 occured 8 and percentage 0.266667\n",
            "Class 2 occured 4 and percentage 0.133333\n",
            "Class 3 occured 13 and percentage 0.433333\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "9307nee9rz4Y",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "load the numpy training and test sets into pytorch Dataset object"
      ]
    },
    {
      "metadata": {
        "id": "gmqg-MVhaHFY",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "cuda = torch.device('cuda')     # Default CUDA device\n",
        "cpu = torch.device('cpu')       # Default CPU device"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "A14LX8bF5GcK",
        "colab_type": "code",
        "outputId": "7c26d4d8-2705-45b7-d108-c13fb5abbfa3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 346
        }
      },
      "cell_type": "code",
      "source": [
        "pd.DataFrame(X_train).describe()"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>...</th>\n",
              "      <th>560</th>\n",
              "      <th>561</th>\n",
              "      <th>562</th>\n",
              "      <th>563</th>\n",
              "      <th>564</th>\n",
              "      <th>565</th>\n",
              "      <th>566</th>\n",
              "      <th>567</th>\n",
              "      <th>568</th>\n",
              "      <th>569</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>30.000000</td>\n",
              "      <td>30.000000</td>\n",
              "      <td>30.000000</td>\n",
              "      <td>30.000000</td>\n",
              "      <td>30.000000</td>\n",
              "      <td>30.000000</td>\n",
              "      <td>30.000000</td>\n",
              "      <td>30.000000</td>\n",
              "      <td>30.000000</td>\n",
              "      <td>30.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>30.000000</td>\n",
              "      <td>30.000000</td>\n",
              "      <td>30.000000</td>\n",
              "      <td>30.000000</td>\n",
              "      <td>30.000000</td>\n",
              "      <td>30.000000</td>\n",
              "      <td>30.000000</td>\n",
              "      <td>30.000000</td>\n",
              "      <td>30.000000</td>\n",
              "      <td>30.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean</th>\n",
              "      <td>-0.611895</td>\n",
              "      <td>-0.610648</td>\n",
              "      <td>-0.606697</td>\n",
              "      <td>-0.601020</td>\n",
              "      <td>-0.593792</td>\n",
              "      <td>-0.585476</td>\n",
              "      <td>-0.577100</td>\n",
              "      <td>-0.568880</td>\n",
              "      <td>-0.561298</td>\n",
              "      <td>-0.555327</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.976487</td>\n",
              "      <td>-0.977011</td>\n",
              "      <td>-0.977404</td>\n",
              "      <td>-0.977851</td>\n",
              "      <td>-0.978500</td>\n",
              "      <td>-0.979447</td>\n",
              "      <td>-0.979884</td>\n",
              "      <td>-0.979993</td>\n",
              "      <td>-0.980466</td>\n",
              "      <td>-0.980602</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>std</th>\n",
              "      <td>0.007103</td>\n",
              "      <td>0.007094</td>\n",
              "      <td>0.007025</td>\n",
              "      <td>0.006931</td>\n",
              "      <td>0.007014</td>\n",
              "      <td>0.007098</td>\n",
              "      <td>0.007005</td>\n",
              "      <td>0.006994</td>\n",
              "      <td>0.007035</td>\n",
              "      <td>0.007000</td>\n",
              "      <td>...</td>\n",
              "      <td>0.006211</td>\n",
              "      <td>0.006145</td>\n",
              "      <td>0.006196</td>\n",
              "      <td>0.006212</td>\n",
              "      <td>0.006270</td>\n",
              "      <td>0.006313</td>\n",
              "      <td>0.006392</td>\n",
              "      <td>0.006372</td>\n",
              "      <td>0.006364</td>\n",
              "      <td>0.006407</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>min</th>\n",
              "      <td>-0.622784</td>\n",
              "      <td>-0.622222</td>\n",
              "      <td>-0.619049</td>\n",
              "      <td>-0.613251</td>\n",
              "      <td>-0.606484</td>\n",
              "      <td>-0.598921</td>\n",
              "      <td>-0.590348</td>\n",
              "      <td>-0.582025</td>\n",
              "      <td>-0.574668</td>\n",
              "      <td>-0.568545</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.995849</td>\n",
              "      <td>-0.996575</td>\n",
              "      <td>-0.997365</td>\n",
              "      <td>-0.997696</td>\n",
              "      <td>-0.998277</td>\n",
              "      <td>-0.999678</td>\n",
              "      <td>-1.000187</td>\n",
              "      <td>-1.000031</td>\n",
              "      <td>-1.000642</td>\n",
              "      <td>-1.001061</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25%</th>\n",
              "      <td>-0.615420</td>\n",
              "      <td>-0.614054</td>\n",
              "      <td>-0.609959</td>\n",
              "      <td>-0.604032</td>\n",
              "      <td>-0.596716</td>\n",
              "      <td>-0.588481</td>\n",
              "      <td>-0.579605</td>\n",
              "      <td>-0.571327</td>\n",
              "      <td>-0.563762</td>\n",
              "      <td>-0.557526</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.978997</td>\n",
              "      <td>-0.979154</td>\n",
              "      <td>-0.979423</td>\n",
              "      <td>-0.979826</td>\n",
              "      <td>-0.980450</td>\n",
              "      <td>-0.981289</td>\n",
              "      <td>-0.982201</td>\n",
              "      <td>-0.982305</td>\n",
              "      <td>-0.982654</td>\n",
              "      <td>-0.982617</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50%</th>\n",
              "      <td>-0.611913</td>\n",
              "      <td>-0.610543</td>\n",
              "      <td>-0.606757</td>\n",
              "      <td>-0.601161</td>\n",
              "      <td>-0.593762</td>\n",
              "      <td>-0.585272</td>\n",
              "      <td>-0.577268</td>\n",
              "      <td>-0.569677</td>\n",
              "      <td>-0.561556</td>\n",
              "      <td>-0.555934</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.975869</td>\n",
              "      <td>-0.976176</td>\n",
              "      <td>-0.976744</td>\n",
              "      <td>-0.977298</td>\n",
              "      <td>-0.977871</td>\n",
              "      <td>-0.978881</td>\n",
              "      <td>-0.979175</td>\n",
              "      <td>-0.979325</td>\n",
              "      <td>-0.979627</td>\n",
              "      <td>-0.980093</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75%</th>\n",
              "      <td>-0.609884</td>\n",
              "      <td>-0.608681</td>\n",
              "      <td>-0.604591</td>\n",
              "      <td>-0.599247</td>\n",
              "      <td>-0.591120</td>\n",
              "      <td>-0.582454</td>\n",
              "      <td>-0.574215</td>\n",
              "      <td>-0.565808</td>\n",
              "      <td>-0.558239</td>\n",
              "      <td>-0.551438</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.972170</td>\n",
              "      <td>-0.973330</td>\n",
              "      <td>-0.973860</td>\n",
              "      <td>-0.974376</td>\n",
              "      <td>-0.974811</td>\n",
              "      <td>-0.975666</td>\n",
              "      <td>-0.976206</td>\n",
              "      <td>-0.976166</td>\n",
              "      <td>-0.976381</td>\n",
              "      <td>-0.976528</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>max</th>\n",
              "      <td>-0.590921</td>\n",
              "      <td>-0.589520</td>\n",
              "      <td>-0.586062</td>\n",
              "      <td>-0.582049</td>\n",
              "      <td>-0.575003</td>\n",
              "      <td>-0.566571</td>\n",
              "      <td>-0.559088</td>\n",
              "      <td>-0.551611</td>\n",
              "      <td>-0.544998</td>\n",
              "      <td>-0.539241</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.968274</td>\n",
              "      <td>-0.968620</td>\n",
              "      <td>-0.968871</td>\n",
              "      <td>-0.969230</td>\n",
              "      <td>-0.970103</td>\n",
              "      <td>-0.971065</td>\n",
              "      <td>-0.971451</td>\n",
              "      <td>-0.971528</td>\n",
              "      <td>-0.971549</td>\n",
              "      <td>-0.971597</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>8 rows × 570 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "             0          1          2          3          4          5    \\\n",
              "count  30.000000  30.000000  30.000000  30.000000  30.000000  30.000000   \n",
              "mean   -0.611895  -0.610648  -0.606697  -0.601020  -0.593792  -0.585476   \n",
              "std     0.007103   0.007094   0.007025   0.006931   0.007014   0.007098   \n",
              "min    -0.622784  -0.622222  -0.619049  -0.613251  -0.606484  -0.598921   \n",
              "25%    -0.615420  -0.614054  -0.609959  -0.604032  -0.596716  -0.588481   \n",
              "50%    -0.611913  -0.610543  -0.606757  -0.601161  -0.593762  -0.585272   \n",
              "75%    -0.609884  -0.608681  -0.604591  -0.599247  -0.591120  -0.582454   \n",
              "max    -0.590921  -0.589520  -0.586062  -0.582049  -0.575003  -0.566571   \n",
              "\n",
              "             6          7          8          9      ...            560  \\\n",
              "count  30.000000  30.000000  30.000000  30.000000    ...      30.000000   \n",
              "mean   -0.577100  -0.568880  -0.561298  -0.555327    ...      -0.976487   \n",
              "std     0.007005   0.006994   0.007035   0.007000    ...       0.006211   \n",
              "min    -0.590348  -0.582025  -0.574668  -0.568545    ...      -0.995849   \n",
              "25%    -0.579605  -0.571327  -0.563762  -0.557526    ...      -0.978997   \n",
              "50%    -0.577268  -0.569677  -0.561556  -0.555934    ...      -0.975869   \n",
              "75%    -0.574215  -0.565808  -0.558239  -0.551438    ...      -0.972170   \n",
              "max    -0.559088  -0.551611  -0.544998  -0.539241    ...      -0.968274   \n",
              "\n",
              "             561        562        563        564        565        566  \\\n",
              "count  30.000000  30.000000  30.000000  30.000000  30.000000  30.000000   \n",
              "mean   -0.977011  -0.977404  -0.977851  -0.978500  -0.979447  -0.979884   \n",
              "std     0.006145   0.006196   0.006212   0.006270   0.006313   0.006392   \n",
              "min    -0.996575  -0.997365  -0.997696  -0.998277  -0.999678  -1.000187   \n",
              "25%    -0.979154  -0.979423  -0.979826  -0.980450  -0.981289  -0.982201   \n",
              "50%    -0.976176  -0.976744  -0.977298  -0.977871  -0.978881  -0.979175   \n",
              "75%    -0.973330  -0.973860  -0.974376  -0.974811  -0.975666  -0.976206   \n",
              "max    -0.968620  -0.968871  -0.969230  -0.970103  -0.971065  -0.971451   \n",
              "\n",
              "             567        568        569  \n",
              "count  30.000000  30.000000  30.000000  \n",
              "mean   -0.979993  -0.980466  -0.980602  \n",
              "std     0.006372   0.006364   0.006407  \n",
              "min    -1.000031  -1.000642  -1.001061  \n",
              "25%    -0.982305  -0.982654  -0.982617  \n",
              "50%    -0.979325  -0.979627  -0.980093  \n",
              "75%    -0.976166  -0.976381  -0.976528  \n",
              "max    -0.971528  -0.971549  -0.971597  \n",
              "\n",
              "[8 rows x 570 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "metadata": {
        "id": "1Wqt26B1oD_g",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "train_ds = create_dataset(X_train, y_train, cuda)\n",
        "test_ds  = create_dataset(X_test, y_test, cuda)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "2uSDLS9-sBYl",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "pass the Dataset objects into a DataLoader"
      ]
    },
    {
      "metadata": {
        "id": "DVMtIjJl6jcr",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class_sample_count = [class_0_count, class_1_count] # dataset has 10 class-1 samples, 1 class-2 samples, etc.\n",
        "weights = 1 / torch.Tensor(class_sample_count)\n",
        "sampler = torch.utils.data.sampler.WeightedRandomSampler(weights, bs)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "5rckvuLErUGq",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "train_dl = DataLoader(train_ds, batch_size=bs, shuffle=False)#, sampler = sampler)\n",
        "test_dl = DataLoader(test_ds, batch_size=bs, shuffle=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "3A86zi48tdPG",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## LSTM-FCN\n",
        "### LSMT block\n",
        "A shuffle layer + LSTM layer + Dropout layer"
      ]
    },
    {
      "metadata": {
        "id": "JDYfUKc8tQhu",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class BlockLSTM(nn.Module):\n",
        "    def __init__(self, time_steps, num_layers, lstm_hs, dropout=0.8, attention=False):\n",
        "        super().__init__()\n",
        "        self.lstm = nn.LSTM(input_size=time_steps, hidden_size=lstm_hs, num_layers=num_layers)\n",
        "        self.dropout = nn.Dropout(p=dropout)\n",
        "    def forward(self, x):\n",
        "        # input is of the form (batch_size, num_layers, time_steps), e.g. (128, 1, 512)\n",
        "        x = torch.transpose(x, 0, 1)\n",
        "        # lstm layer is of the form (num_layers, batch_size, time_steps)\n",
        "        x, (h_n, c_n) = self.lstm(x)\n",
        "        # dropout layer input shape (Sequence Length, Batch Size, Hidden Size * Num Directions)\n",
        "        y = self.dropout(x)\n",
        "        # output shape is same as Dropout intput\n",
        "        return y"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "PskMJhzL8_Ao",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### FCN block\n",
        "\n",
        "#### Convolutional block"
      ]
    },
    {
      "metadata": {
        "id": "4oam7px91HYa",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class BlockFCNConv(nn.Module):\n",
        "    def __init__(self, in_channel=1, out_channel=128, kernel_size=8, momentum=0.99, epsilon=0.001, squeeze=False):\n",
        "        super().__init__()\n",
        "        self.conv = nn.Conv1d(in_channel, out_channel, kernel_size=kernel_size)\n",
        "        self.batch_norm = nn.BatchNorm1d(num_features=out_channel, eps=epsilon, momentum=momentum)\n",
        "        self.relu = nn.ReLU()\n",
        "    def forward(self, x):\n",
        "        # input (batch_size, num_variables, time_steps), e.g. (128, 1, 512)\n",
        "        x = self.conv(x)\n",
        "        # input (batch_size, out_channel, L_out)\n",
        "        x = self.batch_norm(x)\n",
        "        # same shape as input\n",
        "        y = self.relu(x)\n",
        "        return y"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "sxIcU-lx9GeV",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#### FCN block"
      ]
    },
    {
      "metadata": {
        "id": "lNDU3Mij89dR",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class BlockFCN(nn.Module):\n",
        "    def __init__(self, time_steps, channels=[1, 128, 256, 128], kernels=[8, 5, 3], mom=0.99, eps=0.001):\n",
        "        super().__init__()\n",
        "        self.conv1 = BlockFCNConv(channels[0], channels[1], kernels[0], momentum=mom, epsilon=eps, squeeze=True)\n",
        "        self.conv2 = BlockFCNConv(channels[1], channels[2], kernels[1], momentum=mom, epsilon=eps, squeeze=True)\n",
        "        self.conv3 = BlockFCNConv(channels[2], channels[3], kernels[2], momentum=mom, epsilon=eps)\n",
        "        output_size = time_steps - sum(kernels) + len(kernels)\n",
        "        self.global_pooling = nn.AvgPool1d(kernel_size=output_size)\n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = self.conv2(x)\n",
        "        x = self.conv3(x)\n",
        "        # apply Global Average Pooling 1D\n",
        "        y = self.global_pooling(x)\n",
        "        return y"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "mFYGCNCqPQfq",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### LSTM-FCN"
      ]
    },
    {
      "metadata": {
        "id": "QQznEOKCKygx",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class LSTMFCN(nn.Module):\n",
        "    def __init__(self, time_steps, num_variables=1, lstm_hs=256, channels=[1, 128, 256, 128]):\n",
        "        super().__init__()\n",
        "        self.lstm_block = BlockLSTM(time_steps, 1, lstm_hs)\n",
        "        self.fcn_block = BlockFCN(time_steps)\n",
        "        self.dense = nn.Linear(channels[-1] + lstm_hs, num_variables)\n",
        "        self.softmax = nn.LogSoftmax(dim=1) #nn.Softmax(dim=1)\n",
        "    def forward(self, x):\n",
        "        # input is (batch_size, time_steps), it has to be (batch_size, 1, time_steps)\n",
        "        x = x.unsqueeze(1)\n",
        "        # pass input through LSTM block\n",
        "        x1 = self.lstm_block(x)\n",
        "        x1 = torch.squeeze(x1)\n",
        "        # pass input through FCN block\n",
        "        x2 = self.fcn_block(x)\n",
        "        x2 = torch.squeeze(x2)\n",
        "        # concatenate blocks output\n",
        "        x = torch.cat([x1, x2], 1)\n",
        "        # pass through Linear layer\n",
        "        x = self.dense(x)\n",
        "        # pass through Softmax activation\n",
        "        y = x#y = torch.squeeze(x)#y = self.softmax(x)\n",
        "        return y"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ZXyATnq7WOKq",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Training"
      ]
    },
    {
      "metadata": {
        "id": "BCHDSTVzRKGR",
        "colab_type": "code",
        "outputId": "21a3a7ee-1b9e-4808-9067-c41a3c689d76",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "time_steps = X_train.shape[1]\n",
        "num_variables = classes\n",
        "\n",
        "time_steps, num_variables"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(570, 4)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "metadata": {
        "id": "i9eJ3zlrWV7A",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "model = LSTMFCN(time_steps, num_variables).cuda()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "b27y39-dh0Ik",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "look at the different blocks of the Model"
      ]
    },
    {
      "metadata": {
        "id": "i0c4x4NViexX",
        "colab_type": "code",
        "outputId": "83f10e41-c242-4228-d6fc-5d4d999e808d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        }
      },
      "cell_type": "code",
      "source": [
        "# model summary\n",
        "for m in model.children():\n",
        "    print(m.training)#, m)\n",
        "    for j in m.children():\n",
        "        print(j.training, j)"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "True\n",
            "True LSTM(570, 256)\n",
            "True Dropout(p=0.8)\n",
            "True\n",
            "True BlockFCNConv(\n",
            "  (conv): Conv1d(1, 128, kernel_size=(8,), stride=(1,))\n",
            "  (batch_norm): BatchNorm1d(128, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n",
            "  (relu): ReLU()\n",
            ")\n",
            "True BlockFCNConv(\n",
            "  (conv): Conv1d(128, 256, kernel_size=(5,), stride=(1,))\n",
            "  (batch_norm): BatchNorm1d(256, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n",
            "  (relu): ReLU()\n",
            ")\n",
            "True BlockFCNConv(\n",
            "  (conv): Conv1d(256, 128, kernel_size=(3,), stride=(1,))\n",
            "  (batch_norm): BatchNorm1d(128, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n",
            "  (relu): ReLU()\n",
            ")\n",
            "True AvgPool1d(kernel_size=(557,), stride=(557,), padding=(0,))\n",
            "True\n",
            "True\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "xeWqQhn9XPUU",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "look at the parameters (i.e. weights) in each layer"
      ]
    },
    {
      "metadata": {
        "id": "-F4_pcUqW8He",
        "colab_type": "code",
        "outputId": "0f554622-958c-44f5-e24b-3b16d0ff0cb0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 323
        }
      },
      "cell_type": "code",
      "source": [
        "[p.shape for p in model.parameters()]"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[torch.Size([1024, 570]),\n",
              " torch.Size([1024, 256]),\n",
              " torch.Size([1024]),\n",
              " torch.Size([1024]),\n",
              " torch.Size([128, 1, 8]),\n",
              " torch.Size([128]),\n",
              " torch.Size([128]),\n",
              " torch.Size([128]),\n",
              " torch.Size([256, 128, 5]),\n",
              " torch.Size([256]),\n",
              " torch.Size([256]),\n",
              " torch.Size([256]),\n",
              " torch.Size([128, 256, 3]),\n",
              " torch.Size([128]),\n",
              " torch.Size([128]),\n",
              " torch.Size([128]),\n",
              " torch.Size([4, 384]),\n",
              " torch.Size([4])]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    },
    {
      "metadata": {
        "id": "vUO4oNKkxAYs",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Define a learner class to automate the learning process"
      ]
    },
    {
      "metadata": {
        "id": "RnEiGUWHw_kX",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class SimpleLearner():\n",
        "    def __init__(self, data, model, loss_func, wd = 1e-5):\n",
        "        self.data, self.model, self.loss_func = data, model, loss_func\n",
        "        self.wd = wd\n",
        "    \n",
        "    def update_manualgrad(self, x,y,lr):\n",
        "        y_hat = self.model(x)\n",
        "        # weight decay\n",
        "        w2 = 0.\n",
        "        for p in model.parameters(): w2 += (p**2).sum()\n",
        "        # add to regular loss\n",
        "        loss = self.loss_func(y_hat, y) + w2 * self.wd\n",
        "        loss.backward()\n",
        "        with torch.no_grad():\n",
        "            for p in model.parameters():\n",
        "                p.sub_(lr * p.grad)\n",
        "                p.grad.zero_()\n",
        "        return loss.item()\n",
        "\n",
        "    def update(self, x,y,lr):\n",
        "        opt = optim.Adam(self.model.parameters(), lr)\n",
        "        y_hat = self.model(x)\n",
        "        loss = self.loss_func(y_hat, y)\n",
        "        loss.backward()\n",
        "        opt.step()\n",
        "        opt.zero_grad()\n",
        "        return loss.item()\n",
        "\n",
        "    def fit(self, epochs=1, lr=1e-3):\n",
        "        \"\"\"Train the model\"\"\"\n",
        "        losses = []\n",
        "        for i in tqdm(range(epochs)):\n",
        "            for x,y in self.data[0]:\n",
        "                current_loss = self.update(x, y , lr)\n",
        "                losses.append(current_loss)\n",
        "        return losses\n",
        "    \n",
        "    def evaluate(self, X):\n",
        "        \"\"\"Evaluate the given data loader on the model and return predictions\"\"\"\n",
        "        result = None\n",
        "        for x, y in X:\n",
        "            y_hat = self.model(x)\n",
        "            result = y_hat if result is None else np.concatenate((result, y_hat), axis=0)\n",
        "        return result"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "LlRJFy3ikSwQ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#num_variables = 1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "61O6OGUTqkoK",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "model = LSTMFCN(time_steps, num_variables).cuda()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "txh5ujbKXhuz",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "train the model using the DataLoader"
      ]
    },
    {
      "metadata": {
        "id": "GMAfJH9KAY0h",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# depending on the number of classes, use a Binary Cross Entropy or a Negative Log Likelihood loss for more than two classes\n",
        "#loss_func = nn.NLLLoss().cuda() # weight=weights\n",
        "#loss_func = nn.BCEWithLogitsLoss().cuda()\n",
        "loss_func = nn.CrossEntropyLoss().cuda()\n",
        "acc_func = accuracy # accuracy_thresh"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "xxVx2A1pXtuY",
        "colab_type": "code",
        "outputId": "0d74ed78-6a00-4a31-c5ba-3cce19176591",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "lr = 2e-2\n",
        "learner = SimpleLearner([train_dl, test_dl], model, loss_func)\n",
        "losses = learner.fit(10)"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 10/10 [00:00<00:00, 24.67it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "4W1s1mql_ELt",
        "colab_type": "code",
        "outputId": "5841c73b-6b26-43eb-d860-67c1db16157e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        }
      },
      "cell_type": "code",
      "source": [
        "plt.plot(losses)"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<matplotlib.lines.Line2D at 0x7f3879808320>]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 34
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAD4CAYAAAAXUaZHAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAAAASUVORK5CYII=\n",
            "text/plain": [
              "<matplotlib.figure.Figure at 0x7f3879896eb8>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "XyvN_-cSmWPx",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "y_pred = learner.evaluate(test_dl)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "HiVoIWUqp268",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 351
        },
        "outputId": "6ff7961e-8587-4425-a1d3-8d6112c73e98"
      },
      "cell_type": "code",
      "source": [
        "if num_variables==2:\n",
        "    ((y_test - y_pred.argmax(axis=1))**2).mean()\n",
        "else:\n",
        "    ((y_test - y_pred)**2).mean()"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-36-175ac6588682>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_test\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_test\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/tensor.py\u001b[0m in \u001b[0;36m__rsub__\u001b[0;34m(self, other)\u001b[0m\n\u001b[1;32m    361\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    362\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__rsub__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mother\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 363\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_VariableFunctions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrsub\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mother\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    364\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    365\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__rdiv__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mother\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: rsub() received an invalid combination of arguments - got (Tensor, numpy.ndarray), but expected one of:\n * (Tensor input, Tensor other, Number alpha)\n * (Tensor input, Number other, Number alpha)\n"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "jEA7dKEUmP_e",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#### Training with fastai"
      ]
    },
    {
      "metadata": {
        "id": "r3LLF5X9mPJ5",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "model = LSTMFCN(time_steps, num_variables, lstm_hs=128, channels=[1, 128, 128, 128]).cuda()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "IDwkjDH5rXCy",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "data = DataBunch(train_dl=train_dl, valid_dl=test_dl, path=path)\n",
        "learner = Learner(data, model, loss_func=loss_func, metrics=acc_func, wd=0.1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Jhv_-NkEknVO",
        "colab_type": "code",
        "outputId": "5c66dc84-1c92-43cd-f8da-c5a85cfb94f4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 2429
        }
      },
      "cell_type": "code",
      "source": [
        "learner.unfreeze()\n",
        "learner.lr_find()\n",
        "learner.recorder.plot()"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "        <style>\n",
              "        \t/* Turns off some styling */\n",
              "        \tprogress {\n",
              "\n",
              "            \t/* gets rid of default border in Firefox and Opera. */\n",
              "            \tborder: none;\n",
              "\n",
              "            \t/* Needs to be in here for Safari polyfill so background images work as expected. */\n",
              "            \tbackground-size: auto;\n",
              "            }\n",
              "\n",
              "            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
              "                background: #F44336;\n",
              "            }\n",
              "        </style>\n",
              "      <progress value='95' class='' max='100', style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      95.00% [95/100 00:03<00:00]\n",
              "    </div>\n",
              "    \n",
              "<table style='width:300px; margin-bottom:10px'>\n",
              "  <tr>\n",
              "    <th>epoch</th>\n",
              "    <th>train_loss</th>\n",
              "    <th>valid_loss</th>\n",
              "    <th>accuracy</th>\n",
              "  </tr>\n",
              "  <tr>\n",
              "    <th>1</th>\n",
              "    <th>1.406636</th>\n",
              "  </tr>\n",
              "  <tr>\n",
              "    <th>2</th>\n",
              "    <th>1.411034</th>\n",
              "  </tr>\n",
              "  <tr>\n",
              "    <th>3</th>\n",
              "    <th>1.409985</th>\n",
              "  </tr>\n",
              "  <tr>\n",
              "    <th>4</th>\n",
              "    <th>1.406493</th>\n",
              "  </tr>\n",
              "  <tr>\n",
              "    <th>5</th>\n",
              "    <th>1.410553</th>\n",
              "  </tr>\n",
              "  <tr>\n",
              "    <th>6</th>\n",
              "    <th>1.415294</th>\n",
              "  </tr>\n",
              "  <tr>\n",
              "    <th>7</th>\n",
              "    <th>1.411831</th>\n",
              "  </tr>\n",
              "  <tr>\n",
              "    <th>8</th>\n",
              "    <th>1.410828</th>\n",
              "  </tr>\n",
              "  <tr>\n",
              "    <th>9</th>\n",
              "    <th>1.404152</th>\n",
              "  </tr>\n",
              "  <tr>\n",
              "    <th>10</th>\n",
              "    <th>1.408324</th>\n",
              "  </tr>\n",
              "  <tr>\n",
              "    <th>11</th>\n",
              "    <th>1.411163</th>\n",
              "  </tr>\n",
              "  <tr>\n",
              "    <th>12</th>\n",
              "    <th>1.412409</th>\n",
              "  </tr>\n",
              "  <tr>\n",
              "    <th>13</th>\n",
              "    <th>1.411525</th>\n",
              "  </tr>\n",
              "  <tr>\n",
              "    <th>14</th>\n",
              "    <th>1.411862</th>\n",
              "  </tr>\n",
              "  <tr>\n",
              "    <th>15</th>\n",
              "    <th>1.410454</th>\n",
              "  </tr>\n",
              "  <tr>\n",
              "    <th>16</th>\n",
              "    <th>1.409773</th>\n",
              "  </tr>\n",
              "  <tr>\n",
              "    <th>17</th>\n",
              "    <th>1.408035</th>\n",
              "  </tr>\n",
              "  <tr>\n",
              "    <th>18</th>\n",
              "    <th>1.410043</th>\n",
              "  </tr>\n",
              "  <tr>\n",
              "    <th>19</th>\n",
              "    <th>1.409773</th>\n",
              "  </tr>\n",
              "  <tr>\n",
              "    <th>20</th>\n",
              "    <th>1.410705</th>\n",
              "  </tr>\n",
              "  <tr>\n",
              "    <th>21</th>\n",
              "    <th>1.411718</th>\n",
              "  </tr>\n",
              "  <tr>\n",
              "    <th>22</th>\n",
              "    <th>1.412498</th>\n",
              "  </tr>\n",
              "  <tr>\n",
              "    <th>23</th>\n",
              "    <th>1.410445</th>\n",
              "  </tr>\n",
              "  <tr>\n",
              "    <th>24</th>\n",
              "    <th>1.409482</th>\n",
              "  </tr>\n",
              "  <tr>\n",
              "    <th>25</th>\n",
              "    <th>1.408308</th>\n",
              "  </tr>\n",
              "  <tr>\n",
              "    <th>26</th>\n",
              "    <th>1.407267</th>\n",
              "  </tr>\n",
              "  <tr>\n",
              "    <th>27</th>\n",
              "    <th>1.405875</th>\n",
              "  </tr>\n",
              "  <tr>\n",
              "    <th>28</th>\n",
              "    <th>1.407083</th>\n",
              "  </tr>\n",
              "  <tr>\n",
              "    <th>29</th>\n",
              "    <th>1.402903</th>\n",
              "  </tr>\n",
              "  <tr>\n",
              "    <th>30</th>\n",
              "    <th>1.403754</th>\n",
              "  </tr>\n",
              "  <tr>\n",
              "    <th>31</th>\n",
              "    <th>1.400635</th>\n",
              "  </tr>\n",
              "  <tr>\n",
              "    <th>32</th>\n",
              "    <th>1.398681</th>\n",
              "  </tr>\n",
              "  <tr>\n",
              "    <th>33</th>\n",
              "    <th>1.398309</th>\n",
              "  </tr>\n",
              "  <tr>\n",
              "    <th>34</th>\n",
              "    <th>1.399364</th>\n",
              "  </tr>\n",
              "  <tr>\n",
              "    <th>35</th>\n",
              "    <th>1.398536</th>\n",
              "  </tr>\n",
              "  <tr>\n",
              "    <th>36</th>\n",
              "    <th>1.395979</th>\n",
              "  </tr>\n",
              "  <tr>\n",
              "    <th>37</th>\n",
              "    <th>1.395185</th>\n",
              "  </tr>\n",
              "  <tr>\n",
              "    <th>38</th>\n",
              "    <th>1.396499</th>\n",
              "  </tr>\n",
              "  <tr>\n",
              "    <th>39</th>\n",
              "    <th>1.394187</th>\n",
              "  </tr>\n",
              "  <tr>\n",
              "    <th>40</th>\n",
              "    <th>1.393511</th>\n",
              "  </tr>\n",
              "  <tr>\n",
              "    <th>41</th>\n",
              "    <th>1.390119</th>\n",
              "  </tr>\n",
              "  <tr>\n",
              "    <th>42</th>\n",
              "    <th>1.387247</th>\n",
              "  </tr>\n",
              "  <tr>\n",
              "    <th>43</th>\n",
              "    <th>1.385251</th>\n",
              "  </tr>\n",
              "  <tr>\n",
              "    <th>44</th>\n",
              "    <th>1.382787</th>\n",
              "  </tr>\n",
              "  <tr>\n",
              "    <th>45</th>\n",
              "    <th>1.379407</th>\n",
              "  </tr>\n",
              "  <tr>\n",
              "    <th>46</th>\n",
              "    <th>1.377016</th>\n",
              "  </tr>\n",
              "  <tr>\n",
              "    <th>47</th>\n",
              "    <th>1.374868</th>\n",
              "  </tr>\n",
              "  <tr>\n",
              "    <th>48</th>\n",
              "    <th>1.372463</th>\n",
              "  </tr>\n",
              "  <tr>\n",
              "    <th>49</th>\n",
              "    <th>1.370700</th>\n",
              "  </tr>\n",
              "  <tr>\n",
              "    <th>50</th>\n",
              "    <th>1.367687</th>\n",
              "  </tr>\n",
              "  <tr>\n",
              "    <th>51</th>\n",
              "    <th>1.364675</th>\n",
              "  </tr>\n",
              "  <tr>\n",
              "    <th>52</th>\n",
              "    <th>1.364144</th>\n",
              "  </tr>\n",
              "  <tr>\n",
              "    <th>53</th>\n",
              "    <th>1.361970</th>\n",
              "  </tr>\n",
              "  <tr>\n",
              "    <th>54</th>\n",
              "    <th>1.359007</th>\n",
              "  </tr>\n",
              "  <tr>\n",
              "    <th>55</th>\n",
              "    <th>1.358131</th>\n",
              "  </tr>\n",
              "  <tr>\n",
              "    <th>56</th>\n",
              "    <th>1.357273</th>\n",
              "  </tr>\n",
              "  <tr>\n",
              "    <th>57</th>\n",
              "    <th>1.354990</th>\n",
              "  </tr>\n",
              "  <tr>\n",
              "    <th>58</th>\n",
              "    <th>1.352324</th>\n",
              "  </tr>\n",
              "  <tr>\n",
              "    <th>59</th>\n",
              "    <th>1.350721</th>\n",
              "  </tr>\n",
              "  <tr>\n",
              "    <th>60</th>\n",
              "    <th>1.349188</th>\n",
              "  </tr>\n",
              "  <tr>\n",
              "    <th>61</th>\n",
              "    <th>1.349440</th>\n",
              "  </tr>\n",
              "  <tr>\n",
              "    <th>62</th>\n",
              "    <th>1.351508</th>\n",
              "  </tr>\n",
              "  <tr>\n",
              "    <th>63</th>\n",
              "    <th>1.351258</th>\n",
              "  </tr>\n",
              "  <tr>\n",
              "    <th>64</th>\n",
              "    <th>1.349927</th>\n",
              "  </tr>\n",
              "  <tr>\n",
              "    <th>65</th>\n",
              "    <th>1.350420</th>\n",
              "  </tr>\n",
              "  <tr>\n",
              "    <th>66</th>\n",
              "    <th>1.348101</th>\n",
              "  </tr>\n",
              "  <tr>\n",
              "    <th>67</th>\n",
              "    <th>1.346145</th>\n",
              "  </tr>\n",
              "  <tr>\n",
              "    <th>68</th>\n",
              "    <th>1.344843</th>\n",
              "  </tr>\n",
              "  <tr>\n",
              "    <th>69</th>\n",
              "    <th>1.345760</th>\n",
              "  </tr>\n",
              "  <tr>\n",
              "    <th>70</th>\n",
              "    <th>1.350025</th>\n",
              "  </tr>\n",
              "  <tr>\n",
              "    <th>71</th>\n",
              "    <th>1.347955</th>\n",
              "  </tr>\n",
              "  <tr>\n",
              "    <th>72</th>\n",
              "    <th>1.353235</th>\n",
              "  </tr>\n",
              "  <tr>\n",
              "    <th>73</th>\n",
              "    <th>1.355921</th>\n",
              "  </tr>\n",
              "  <tr>\n",
              "    <th>74</th>\n",
              "    <th>1.359981</th>\n",
              "  </tr>\n",
              "  <tr>\n",
              "    <th>75</th>\n",
              "    <th>1.364156</th>\n",
              "  </tr>\n",
              "  <tr>\n",
              "    <th>76</th>\n",
              "    <th>1.366919</th>\n",
              "  </tr>\n",
              "  <tr>\n",
              "    <th>77</th>\n",
              "    <th>1.371681</th>\n",
              "  </tr>\n",
              "  <tr>\n",
              "    <th>78</th>\n",
              "    <th>1.379854</th>\n",
              "  </tr>\n",
              "  <tr>\n",
              "    <th>79</th>\n",
              "    <th>1.385256</th>\n",
              "  </tr>\n",
              "  <tr>\n",
              "    <th>80</th>\n",
              "    <th>1.402090</th>\n",
              "  </tr>\n",
              "  <tr>\n",
              "    <th>81</th>\n",
              "    <th>1.412945</th>\n",
              "  </tr>\n",
              "  <tr>\n",
              "    <th>82</th>\n",
              "    <th>1.430090</th>\n",
              "  </tr>\n",
              "  <tr>\n",
              "    <th>83</th>\n",
              "    <th>1.454959</th>\n",
              "  </tr>\n",
              "  <tr>\n",
              "    <th>84</th>\n",
              "    <th>1.478099</th>\n",
              "  </tr>\n",
              "  <tr>\n",
              "    <th>85</th>\n",
              "    <th>1.486666</th>\n",
              "  </tr>\n",
              "  <tr>\n",
              "    <th>86</th>\n",
              "    <th>1.522096</th>\n",
              "  </tr>\n",
              "  <tr>\n",
              "    <th>87</th>\n",
              "    <th>1.549306</th>\n",
              "  </tr>\n",
              "  <tr>\n",
              "    <th>88</th>\n",
              "    <th>1.603077</th>\n",
              "  </tr>\n",
              "  <tr>\n",
              "    <th>89</th>\n",
              "    <th>1.684205</th>\n",
              "  </tr>\n",
              "  <tr>\n",
              "    <th>90</th>\n",
              "    <th>1.737923</th>\n",
              "  </tr>\n",
              "  <tr>\n",
              "    <th>91</th>\n",
              "    <th>1.814045</th>\n",
              "  </tr>\n",
              "  <tr>\n",
              "    <th>92</th>\n",
              "    <th>1.900465</th>\n",
              "  </tr>\n",
              "  <tr>\n",
              "    <th>93</th>\n",
              "    <th>2.022480</th>\n",
              "  </tr>\n",
              "  <tr>\n",
              "    <th>94</th>\n",
              "    <th>2.339334</th>\n",
              "  </tr>\n",
              "  <tr>\n",
              "    <th>95</th>\n",
              "    <th>2.665863</th>\n",
              "  </tr>\n",
              "  <tr>\n",
              "    <th>96</th>\n",
              "    <th>3.523362</th>\n",
              "  </tr>\n",
              "  <tr>\n",
              "    <th>97</th>\n",
              "    <th>4.600011</th>\n",
              "  </tr>\n",
              "  <tr>\n",
              "    <th>98</th>\n",
              "    <th>5.264989</th>\n",
              "  </tr>\n",
              "  <tr>\n",
              "\n",
              "  </tr>\n",
              "</table>\n",
              "\n",
              "\n",
              "    <div>\n",
              "        <style>\n",
              "        \t/* Turns off some styling */\n",
              "        \tprogress {\n",
              "\n",
              "            \t/* gets rid of default border in Firefox and Opera. */\n",
              "            \tborder: none;\n",
              "\n",
              "            \t/* Needs to be in here for Safari polyfill so background images work as expected. */\n",
              "            \tbackground-size: auto;\n",
              "            }\n",
              "\n",
              "            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
              "                background: #F44336;\n",
              "            }\n",
              "        </style>\n",
              "      <progress value='0' class='progress-bar-interrupted' max='1', style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      Interrupted\n",
              "    </div>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "LR Finder is complete, type {learner_name}.recorder.plot() to see the graph.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYAAAAEGCAYAAABsLkJ6AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nlAJARKRN/X+ChsSUR+pXtQAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<matplotlib.figure.Figure at 0x7f3879856e10>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "Lex4Hkn078kn",
        "colab_type": "code",
        "outputId": "c5947e0f-3331-4a61-8301-ed91397e49e3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1125
        }
      },
      "cell_type": "code",
      "source": [
        "learner.fit(50, lr=3e-3)"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "Total time: 00:03 <p><table style='width:300px; margin-bottom:10px'>\n",
              "  <tr>\n",
              "    <th>epoch</th>\n",
              "    <th>train_loss</th>\n",
              "    <th>valid_loss</th>\n",
              "    <th>accuracy</th>\n",
              "  </tr>\n",
              "  <tr>\n",
              "    <th>1</th>\n",
              "    <th>1.396385</th>\n",
              "    <th>1.359485</th>\n",
              "    <th>0.400000</th>\n",
              "  </tr>\n",
              "  <tr>\n",
              "    <th>2</th>\n",
              "    <th>1.381889</th>\n",
              "    <th>1.357261</th>\n",
              "    <th>0.400000</th>\n",
              "  </tr>\n",
              "  <tr>\n",
              "    <th>3</th>\n",
              "    <th>1.372547</th>\n",
              "    <th>1.333944</th>\n",
              "    <th>0.400000</th>\n",
              "  </tr>\n",
              "  <tr>\n",
              "    <th>4</th>\n",
              "    <th>1.369740</th>\n",
              "    <th>1.322976</th>\n",
              "    <th>0.400000</th>\n",
              "  </tr>\n",
              "  <tr>\n",
              "    <th>5</th>\n",
              "    <th>1.353715</th>\n",
              "    <th>1.312739</th>\n",
              "    <th>0.400000</th>\n",
              "  </tr>\n",
              "  <tr>\n",
              "    <th>6</th>\n",
              "    <th>1.356948</th>\n",
              "    <th>1.308630</th>\n",
              "    <th>0.400000</th>\n",
              "  </tr>\n",
              "  <tr>\n",
              "    <th>7</th>\n",
              "    <th>1.347808</th>\n",
              "    <th>1.304727</th>\n",
              "    <th>0.400000</th>\n",
              "  </tr>\n",
              "  <tr>\n",
              "    <th>8</th>\n",
              "    <th>1.349517</th>\n",
              "    <th>1.301207</th>\n",
              "    <th>0.400000</th>\n",
              "  </tr>\n",
              "  <tr>\n",
              "    <th>9</th>\n",
              "    <th>1.352698</th>\n",
              "    <th>1.298808</th>\n",
              "    <th>0.400000</th>\n",
              "  </tr>\n",
              "  <tr>\n",
              "    <th>10</th>\n",
              "    <th>1.349861</th>\n",
              "    <th>1.297599</th>\n",
              "    <th>0.400000</th>\n",
              "  </tr>\n",
              "  <tr>\n",
              "    <th>11</th>\n",
              "    <th>1.348805</th>\n",
              "    <th>1.296743</th>\n",
              "    <th>0.400000</th>\n",
              "  </tr>\n",
              "  <tr>\n",
              "    <th>12</th>\n",
              "    <th>1.351144</th>\n",
              "    <th>1.296144</th>\n",
              "    <th>0.400000</th>\n",
              "  </tr>\n",
              "  <tr>\n",
              "    <th>13</th>\n",
              "    <th>1.347535</th>\n",
              "    <th>1.295880</th>\n",
              "    <th>0.400000</th>\n",
              "  </tr>\n",
              "  <tr>\n",
              "    <th>14</th>\n",
              "    <th>1.341564</th>\n",
              "    <th>1.295719</th>\n",
              "    <th>0.400000</th>\n",
              "  </tr>\n",
              "  <tr>\n",
              "    <th>15</th>\n",
              "    <th>1.340976</th>\n",
              "    <th>1.295878</th>\n",
              "    <th>0.400000</th>\n",
              "  </tr>\n",
              "  <tr>\n",
              "    <th>16</th>\n",
              "    <th>1.336836</th>\n",
              "    <th>1.295944</th>\n",
              "    <th>0.400000</th>\n",
              "  </tr>\n",
              "  <tr>\n",
              "    <th>17</th>\n",
              "    <th>1.335998</th>\n",
              "    <th>1.296649</th>\n",
              "    <th>0.400000</th>\n",
              "  </tr>\n",
              "  <tr>\n",
              "    <th>18</th>\n",
              "    <th>1.335531</th>\n",
              "    <th>1.297585</th>\n",
              "    <th>0.400000</th>\n",
              "  </tr>\n",
              "  <tr>\n",
              "    <th>19</th>\n",
              "    <th>1.338487</th>\n",
              "    <th>1.298796</th>\n",
              "    <th>0.400000</th>\n",
              "  </tr>\n",
              "  <tr>\n",
              "    <th>20</th>\n",
              "    <th>1.337068</th>\n",
              "    <th>1.300024</th>\n",
              "    <th>0.400000</th>\n",
              "  </tr>\n",
              "  <tr>\n",
              "    <th>21</th>\n",
              "    <th>1.332479</th>\n",
              "    <th>1.300631</th>\n",
              "    <th>0.400000</th>\n",
              "  </tr>\n",
              "  <tr>\n",
              "    <th>22</th>\n",
              "    <th>1.327656</th>\n",
              "    <th>1.301175</th>\n",
              "    <th>0.400000</th>\n",
              "  </tr>\n",
              "  <tr>\n",
              "    <th>23</th>\n",
              "    <th>1.325289</th>\n",
              "    <th>1.301993</th>\n",
              "    <th>0.400000</th>\n",
              "  </tr>\n",
              "  <tr>\n",
              "    <th>24</th>\n",
              "    <th>1.325685</th>\n",
              "    <th>1.302029</th>\n",
              "    <th>0.400000</th>\n",
              "  </tr>\n",
              "  <tr>\n",
              "    <th>25</th>\n",
              "    <th>1.328585</th>\n",
              "    <th>1.302044</th>\n",
              "    <th>0.400000</th>\n",
              "  </tr>\n",
              "  <tr>\n",
              "    <th>26</th>\n",
              "    <th>1.328985</th>\n",
              "    <th>1.301402</th>\n",
              "    <th>0.400000</th>\n",
              "  </tr>\n",
              "  <tr>\n",
              "    <th>27</th>\n",
              "    <th>1.328396</th>\n",
              "    <th>1.299837</th>\n",
              "    <th>0.400000</th>\n",
              "  </tr>\n",
              "  <tr>\n",
              "    <th>28</th>\n",
              "    <th>1.328098</th>\n",
              "    <th>1.298067</th>\n",
              "    <th>0.400000</th>\n",
              "  </tr>\n",
              "  <tr>\n",
              "    <th>29</th>\n",
              "    <th>1.328023</th>\n",
              "    <th>1.296554</th>\n",
              "    <th>0.400000</th>\n",
              "  </tr>\n",
              "  <tr>\n",
              "    <th>30</th>\n",
              "    <th>1.327426</th>\n",
              "    <th>1.295573</th>\n",
              "    <th>0.400000</th>\n",
              "  </tr>\n",
              "  <tr>\n",
              "    <th>31</th>\n",
              "    <th>1.325342</th>\n",
              "    <th>1.295260</th>\n",
              "    <th>0.400000</th>\n",
              "  </tr>\n",
              "  <tr>\n",
              "    <th>32</th>\n",
              "    <th>1.323088</th>\n",
              "    <th>1.295490</th>\n",
              "    <th>0.400000</th>\n",
              "  </tr>\n",
              "  <tr>\n",
              "    <th>33</th>\n",
              "    <th>1.323983</th>\n",
              "    <th>1.295714</th>\n",
              "    <th>0.400000</th>\n",
              "  </tr>\n",
              "  <tr>\n",
              "    <th>34</th>\n",
              "    <th>1.322607</th>\n",
              "    <th>1.296001</th>\n",
              "    <th>0.400000</th>\n",
              "  </tr>\n",
              "  <tr>\n",
              "    <th>35</th>\n",
              "    <th>1.324413</th>\n",
              "    <th>1.296550</th>\n",
              "    <th>0.400000</th>\n",
              "  </tr>\n",
              "  <tr>\n",
              "    <th>36</th>\n",
              "    <th>1.323503</th>\n",
              "    <th>1.297314</th>\n",
              "    <th>0.400000</th>\n",
              "  </tr>\n",
              "  <tr>\n",
              "    <th>37</th>\n",
              "    <th>1.323562</th>\n",
              "    <th>1.298818</th>\n",
              "    <th>0.400000</th>\n",
              "  </tr>\n",
              "  <tr>\n",
              "    <th>38</th>\n",
              "    <th>1.321722</th>\n",
              "    <th>1.300533</th>\n",
              "    <th>0.400000</th>\n",
              "  </tr>\n",
              "  <tr>\n",
              "    <th>39</th>\n",
              "    <th>1.320682</th>\n",
              "    <th>1.300425</th>\n",
              "    <th>0.400000</th>\n",
              "  </tr>\n",
              "  <tr>\n",
              "    <th>40</th>\n",
              "    <th>1.319266</th>\n",
              "    <th>1.299781</th>\n",
              "    <th>0.400000</th>\n",
              "  </tr>\n",
              "  <tr>\n",
              "    <th>41</th>\n",
              "    <th>1.318973</th>\n",
              "    <th>1.298638</th>\n",
              "    <th>0.400000</th>\n",
              "  </tr>\n",
              "  <tr>\n",
              "    <th>42</th>\n",
              "    <th>1.317428</th>\n",
              "    <th>1.297718</th>\n",
              "    <th>0.400000</th>\n",
              "  </tr>\n",
              "  <tr>\n",
              "    <th>43</th>\n",
              "    <th>1.316124</th>\n",
              "    <th>1.296753</th>\n",
              "    <th>0.400000</th>\n",
              "  </tr>\n",
              "  <tr>\n",
              "    <th>44</th>\n",
              "    <th>1.315131</th>\n",
              "    <th>1.296208</th>\n",
              "    <th>0.400000</th>\n",
              "  </tr>\n",
              "  <tr>\n",
              "    <th>45</th>\n",
              "    <th>1.313794</th>\n",
              "    <th>1.295826</th>\n",
              "    <th>0.400000</th>\n",
              "  </tr>\n",
              "  <tr>\n",
              "    <th>46</th>\n",
              "    <th>1.314028</th>\n",
              "    <th>1.295697</th>\n",
              "    <th>0.400000</th>\n",
              "  </tr>\n",
              "  <tr>\n",
              "    <th>47</th>\n",
              "    <th>1.312592</th>\n",
              "    <th>1.295847</th>\n",
              "    <th>0.400000</th>\n",
              "  </tr>\n",
              "  <tr>\n",
              "    <th>48</th>\n",
              "    <th>1.311265</th>\n",
              "    <th>1.296315</th>\n",
              "    <th>0.400000</th>\n",
              "  </tr>\n",
              "  <tr>\n",
              "    <th>49</th>\n",
              "    <th>1.309709</th>\n",
              "    <th>1.296983</th>\n",
              "    <th>0.400000</th>\n",
              "  </tr>\n",
              "  <tr>\n",
              "    <th>50</th>\n",
              "    <th>1.308661</th>\n",
              "    <th>1.297208</th>\n",
              "    <th>0.400000</th>\n",
              "  </tr>\n",
              "  <tr>\n",
              "\n",
              "  </tr>\n",
              "</table>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "mgQq0yIpiwC_",
        "colab_type": "code",
        "outputId": "9f372981-5379-4ec7-8749-ea9cb20625bb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 285
        }
      },
      "cell_type": "code",
      "source": [
        "learner.fit_one_cycle(10)"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "Total time: 00:00 <p><table style='width:300px; margin-bottom:10px'>\n",
              "  <tr>\n",
              "    <th>epoch</th>\n",
              "    <th>train_loss</th>\n",
              "    <th>valid_loss</th>\n",
              "    <th>accuracy</th>\n",
              "  </tr>\n",
              "  <tr>\n",
              "    <th>1</th>\n",
              "    <th>1.262412</th>\n",
              "    <th>1.297237</th>\n",
              "    <th>0.400000</th>\n",
              "  </tr>\n",
              "  <tr>\n",
              "    <th>2</th>\n",
              "    <th>1.276047</th>\n",
              "    <th>1.296188</th>\n",
              "    <th>0.400000</th>\n",
              "  </tr>\n",
              "  <tr>\n",
              "    <th>3</th>\n",
              "    <th>1.273078</th>\n",
              "    <th>1.294888</th>\n",
              "    <th>0.400000</th>\n",
              "  </tr>\n",
              "  <tr>\n",
              "    <th>4</th>\n",
              "    <th>1.280526</th>\n",
              "    <th>1.299112</th>\n",
              "    <th>0.400000</th>\n",
              "  </tr>\n",
              "  <tr>\n",
              "    <th>5</th>\n",
              "    <th>1.283353</th>\n",
              "    <th>1.303249</th>\n",
              "    <th>0.400000</th>\n",
              "  </tr>\n",
              "  <tr>\n",
              "    <th>6</th>\n",
              "    <th>1.301668</th>\n",
              "    <th>1.297865</th>\n",
              "    <th>0.400000</th>\n",
              "  </tr>\n",
              "  <tr>\n",
              "    <th>7</th>\n",
              "    <th>1.298818</th>\n",
              "    <th>1.295340</th>\n",
              "    <th>0.400000</th>\n",
              "  </tr>\n",
              "  <tr>\n",
              "    <th>8</th>\n",
              "    <th>1.296389</th>\n",
              "    <th>1.294743</th>\n",
              "    <th>0.400000</th>\n",
              "  </tr>\n",
              "  <tr>\n",
              "    <th>9</th>\n",
              "    <th>1.285329</th>\n",
              "    <th>1.294697</th>\n",
              "    <th>0.400000</th>\n",
              "  </tr>\n",
              "  <tr>\n",
              "    <th>10</th>\n",
              "    <th>1.283528</th>\n",
              "    <th>1.294878</th>\n",
              "    <th>0.400000</th>\n",
              "  </tr>\n",
              "  <tr>\n",
              "\n",
              "  </tr>\n",
              "</table>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "lVO2pL6br0n1",
        "colab_type": "code",
        "outputId": "73e342cf-ea1e-493d-a46c-75ef2e9d336d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 561
        }
      },
      "cell_type": "code",
      "source": [
        "learner.get_preds(DatasetType.Valid)"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[tensor([[0.1616, 0.2780, 0.1322, 0.4282],\n",
              "         [0.1625, 0.2776, 0.1329, 0.4270],\n",
              "         [0.1632, 0.2772, 0.1329, 0.4267],\n",
              "         [0.1628, 0.2773, 0.1328, 0.4271],\n",
              "         [0.1626, 0.2775, 0.1327, 0.4273],\n",
              "         [0.1615, 0.2780, 0.1322, 0.4283],\n",
              "         [0.1618, 0.2779, 0.1323, 0.4280],\n",
              "         [0.1622, 0.2777, 0.1325, 0.4275],\n",
              "         [0.1623, 0.2776, 0.1325, 0.4276],\n",
              "         [0.1615, 0.2780, 0.1321, 0.4283],\n",
              "         [0.1621, 0.2777, 0.1324, 0.4278],\n",
              "         [0.1633, 0.2772, 0.1330, 0.4265],\n",
              "         [0.1618, 0.2779, 0.1323, 0.4279],\n",
              "         [0.1622, 0.2777, 0.1325, 0.4276],\n",
              "         [0.1612, 0.2781, 0.1319, 0.4288],\n",
              "         [0.1652, 0.2764, 0.1352, 0.4231],\n",
              "         [0.1623, 0.2777, 0.1331, 0.4269],\n",
              "         [0.1624, 0.2776, 0.1328, 0.4273],\n",
              "         [0.1613, 0.2780, 0.1319, 0.4288],\n",
              "         [0.1610, 0.2781, 0.1318, 0.4291],\n",
              "         [0.1620, 0.2776, 0.1322, 0.4281],\n",
              "         [0.1615, 0.2779, 0.1320, 0.4287],\n",
              "         [0.1616, 0.2778, 0.1320, 0.4286],\n",
              "         [0.1613, 0.2780, 0.1319, 0.4288],\n",
              "         [0.1622, 0.2775, 0.1322, 0.4281],\n",
              "         [0.1610, 0.2782, 0.1317, 0.4291],\n",
              "         [0.1620, 0.2776, 0.1322, 0.4282],\n",
              "         [0.1617, 0.2777, 0.1321, 0.4284],\n",
              "         [0.1626, 0.2773, 0.1325, 0.4275],\n",
              "         [0.1614, 0.2782, 0.1321, 0.4283]]),\n",
              " tensor([0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3,\n",
              "         3, 3, 3, 3, 3, 3])]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 42
        }
      ]
    },
    {
      "metadata": {
        "id": "2q4KDLq8smva",
        "colab_type": "code",
        "outputId": "2cff59be-1e80-4106-f5e1-55d4277b9135",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 561
        }
      },
      "cell_type": "code",
      "source": [
        "learner.get_preds(DatasetType.Train)"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[tensor([[0.1642, 0.2768, 0.1338, 0.4252],\n",
              "         [0.1633, 0.2772, 0.1331, 0.4264],\n",
              "         [0.1634, 0.2770, 0.1331, 0.4264],\n",
              "         [0.1623, 0.2776, 0.1324, 0.4276],\n",
              "         [0.1638, 0.2770, 0.1333, 0.4260],\n",
              "         [0.1625, 0.2775, 0.1328, 0.4272],\n",
              "         [0.1620, 0.2778, 0.1324, 0.4277],\n",
              "         [0.1619, 0.2779, 0.1324, 0.4279],\n",
              "         [0.1620, 0.2777, 0.1326, 0.4277],\n",
              "         [0.1621, 0.2777, 0.1324, 0.4277],\n",
              "         [0.1612, 0.2783, 0.1321, 0.4284],\n",
              "         [0.1618, 0.2780, 0.1324, 0.4278],\n",
              "         [0.1623, 0.2777, 0.1325, 0.4276],\n",
              "         [0.1605, 0.2785, 0.1318, 0.4292],\n",
              "         [0.1609, 0.2783, 0.1319, 0.4289],\n",
              "         [0.1658, 0.2760, 0.1354, 0.4228],\n",
              "         [0.1657, 0.2762, 0.1355, 0.4226],\n",
              "         [0.1616, 0.2779, 0.1322, 0.4283],\n",
              "         [0.1617, 0.2778, 0.1320, 0.4285],\n",
              "         [0.1616, 0.2778, 0.1320, 0.4286],\n",
              "         [0.1609, 0.2782, 0.1318, 0.4291],\n",
              "         [0.1619, 0.2777, 0.1321, 0.4283],\n",
              "         [0.1614, 0.2780, 0.1320, 0.4287],\n",
              "         [0.1611, 0.2781, 0.1319, 0.4289],\n",
              "         [0.1623, 0.2775, 0.1325, 0.4278],\n",
              "         [0.1616, 0.2778, 0.1321, 0.4285],\n",
              "         [0.1618, 0.2777, 0.1321, 0.4284],\n",
              "         [0.1622, 0.2776, 0.1324, 0.4278],\n",
              "         [0.1619, 0.2776, 0.1321, 0.4283],\n",
              "         [0.1618, 0.2779, 0.1322, 0.4281]]),\n",
              " tensor([0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3,\n",
              "         3, 3, 3, 3, 3, 3])]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 43
        }
      ]
    },
    {
      "metadata": {
        "id": "6pF4Hwx3oR4d",
        "colab_type": "code",
        "outputId": "104754db-87b4-46cf-8ccc-f0de870593ce",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 283
        }
      },
      "cell_type": "code",
      "source": [
        "learner.predict(data)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-108-6dfa2e025c4e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mlearner\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/fastai/basic_train.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, img, pbar)\u001b[0m\n\u001b[1;32m    223\u001b[0m         \u001b[0;34m\"Return prect class, label and probabilities for `img`.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    224\u001b[0m         \u001b[0mds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msingle_dl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 225\u001b[0;31m         \u001b[0mds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_item\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    226\u001b[0m         \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpred_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mds_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mDatasetType\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSingle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpbar\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpbar\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    227\u001b[0m         \u001b[0mds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclear_item\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'TensorDataset' object has no attribute 'set_item'"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "QokS_SU5sAs-",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#### Training one cycle"
      ]
    },
    {
      "metadata": {
        "id": "nwkS7lQ6Nbq8",
        "colab_type": "code",
        "outputId": "8cde1ed5-5b26-43c9-8ca5-99b0b787952c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        }
      },
      "cell_type": "code",
      "source": [
        "data = DataBunch(train_dl=train_dl, valid_dl=test_dl, path=path)\n",
        "learner = Learner(data, model, loss_func=loss_func, metrics=accuracy)\n",
        "learner.fit(10, lr=5e-5)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total time: 00:03\n",
            "epoch  train_loss  valid_loss  accuracy\n",
            "1      0.003275    0.785197    0.762590  (00:00)\n",
            "2      0.003349    0.800480    0.762590  (00:00)\n",
            "3      0.003107    0.804093    0.762590  (00:00)\n",
            "4      0.002767    0.812344    0.762590  (00:00)\n",
            "5      0.002825    0.806561    0.762590  (00:00)\n",
            "6      0.002692    0.815485    0.755396  (00:00)\n",
            "7      0.002587    0.823958    0.755396  (00:00)\n",
            "8      0.002495    0.824159    0.755396  (00:00)\n",
            "9      0.002463    0.840908    0.755396  (00:00)\n",
            "10     0.002364    0.839903    0.755396  (00:00)\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "b8Pcr02Khxsy",
        "colab_type": "code",
        "outputId": "fe0ed6b4-c1e6-424e-88b4-780c223ceb44",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 459
        }
      },
      "cell_type": "code",
      "source": [
        "model.eval()\n",
        "model.train()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LSTMFCN(\n",
              "  (lstm_block): BlockLSTM(\n",
              "    (lstm): LSTM(512, 256)\n",
              "    (dropout): Dropout(p=0.8)\n",
              "  )\n",
              "  (fcn_block): BlockFCN(\n",
              "    (conv1): BlockFCNConv(\n",
              "      (conv): Conv1d(1, 128, kernel_size=(8,), stride=(1,))\n",
              "      (batch_norm): BatchNorm1d(128, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU()\n",
              "    )\n",
              "    (conv2): BlockFCNConv(\n",
              "      (conv): Conv1d(128, 256, kernel_size=(5,), stride=(1,))\n",
              "      (batch_norm): BatchNorm1d(256, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU()\n",
              "    )\n",
              "    (conv3): BlockFCNConv(\n",
              "      (conv): Conv1d(256, 128, kernel_size=(3,), stride=(1,))\n",
              "      (batch_norm): BatchNorm1d(128, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU()\n",
              "    )\n",
              "    (global_pooling): AvgPool1d(kernel_size=(499,), stride=(499,), padding=(0,))\n",
              "  )\n",
              "  (dense): Linear(in_features=384, out_features=2, bias=True)\n",
              "  (softmax): LogSoftmax()\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 201
        }
      ]
    },
    {
      "metadata": {
        "id": "mTTetDTNVH-p",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}